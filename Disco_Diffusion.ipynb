{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !zip -r \"/content/drive/MyDrive/AI/Disco_Diffusion/DDT-$(date +\"%Y-%m-%d\").zip\" /content/drive/MyDrive/AI/Disco_Diffusion/images_out\n",
    "except:\n",
    "    print(\"Not in Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !rm -rf '/content/drive/MyDrive/AI/Disco_Diffusion/images_out'\n",
    "except:\n",
    "    print(\"Not in Colab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SetupTop"
   },
   "source": [
    "# 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "CheckGPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 20 16:45:47 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:0A:00.0  On |                  N/A |\r\n",
      "|  0%   51C    P3    56W / 380W |   1151MiB / 10240MiB |     10%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1012    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "|    0   N/A  N/A      1176    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "|    0   N/A  N/A      1500    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      2844    C+G   ...signal-desktop\\Signal.exe    N/A      |\r\n",
      "|    0   N/A  N/A      7296    C+G   ...s\\Notepad++\\notepad++.exe    N/A      |\r\n",
      "|    0   N/A  N/A      7392    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      7944    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n",
      "|    0   N/A  N/A     11124    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n",
      "|    0   N/A  N/A     11148    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\r\n",
      "|    0   N/A  N/A     11916    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n",
      "|    0   N/A  N/A     12128    C+G   ...\\app-1.0.9005\\Discord.exe    N/A      |\r\n",
      "|    0   N/A  N/A     12280    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A     12780    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\n",
      "ECC features not supported for GPU 00000000:0A:00.0.\r\n",
      "Treating as warning and moving on.\r\n",
      "All done.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title 1.1 Check GPU Status\n",
    "import subprocess\n",
    "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
    "if simple_nvidia_smi_display:\n",
    "    #!nvidia-smi\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "else:\n",
    "    #!nvidia-smi -i 0 -e 0\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_ecc_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "PrepFolders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Colab not detected.\n"
     ]
    }
   ],
   "source": [
    "#@title 1.2 Prepare Folders\n",
    "import subprocess, os, sys, ipykernel, requests\n",
    "\n",
    "def gitclone(url, targetdir=None):\n",
    "    if targetdir:\n",
    "        res = subprocess.run(['git', 'clone', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def fetchsave(url, path):\n",
    "    if is_colab:\n",
    "        res = subprocess.run(['wget', url, '-O', f'{path}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "        print(res)\n",
    "    else:\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(fetch(url).getbuffer())\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith(\n",
    "            'https://'):\n",
    "        print(f'Fetching {str(url_or_path)}. \\nThis might take a while... please wait.')\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def pipi(modulestr):\n",
    "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def pipie(modulestr):\n",
    "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"Google Colab detected. Using Google Drive.\")\n",
    "    is_colab = True\n",
    "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
    "    google_drive = True #@param {type:\"boolean\"}\n",
    "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
    "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
    "except:\n",
    "    is_colab = False\n",
    "    google_drive = False\n",
    "    save_models_to_google_drive = False\n",
    "    print(\"Google Colab not detected.\")\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive is True:\n",
    "        drive.mount('/content/drive')\n",
    "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
    "    else:\n",
    "        root_path = '/content'\n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "\n",
    "import os\n",
    "def createPath(filepath):\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "initDirPath = f'{root_path}/init_images'\n",
    "createPath(initDirPath)\n",
    "outDirPath = f'{root_path}/images_out'\n",
    "createPath(outDirPath)\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
    "        model_path = '/content/models'\n",
    "        createPath(model_path)\n",
    "    if google_drive and save_models_to_google_drive:\n",
    "        model_path = f'{root_path}/models'\n",
    "        createPath(model_path)\n",
    "else:\n",
    "    model_path = f'{root_path}/models'\n",
    "    createPath(model_path)\n",
    "\n",
    "# libraries = f'{root_path}/libraries'\n",
    "# createPath(libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "InstallDeps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lpips in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.1.4)\r\n",
      "Requirement already satisfied: datetime in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (4.5)\r\n",
      "Requirement already satisfied: timm in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.6.5)\r\n",
      "Requirement already satisfied: ftfy in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (6.1.1)\r\n",
      "Requirement already satisfied: einops in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.4.1)\r\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (1.6.5)\r\n",
      "Requirement already satisfied: omegaconf in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (2.2.2)\r\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (4.64.0)\r\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (0.11.3+cu113)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.10.2+cu113)\r\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.21.6)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.7.3)\r\n",
      "Requirement already satisfied: pytz in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (2022.1)\r\n",
      "Requirement already satisfied: zope.interface in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (5.4.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from ftfy) (0.2.5)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.3.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2022.5.0)\r\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (4.3.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (3.19.4)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.9.2)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from omegaconf) (4.9.3)\r\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\r\n",
      "Requirement already satisfied: requests in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.28.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.1.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\r\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.2)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (61.2.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.47.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from torchvision>=0.2.1->lpips) (9.2.0)\r\n",
      "Requirement already satisfied: colorama in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.2.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.10)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#@title ### 1.3 Install, import dependencies and set up runtime devices\n",
    "\n",
    "import pathlib, shutil, os, sys\n",
    "\n",
    "# There are some reports that with a T4 or V100 on Colab, downgrading to a previous version of PyTorch may be necessary.\n",
    "# .. but there are also reports that downgrading breaks them!  If you're facing issues, you may want to try uncommenting and running this code.\n",
    "# nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "# cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
    "# if is_colab:\n",
    "#     if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
    "#         print(\"Downgrading pytorch. This can take a couple minutes ...\")\n",
    "#         downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "#         print(\"pytorch downgraded.\")\n",
    "\n",
    "#@markdown Check this if you want to use CPU\n",
    "useCPU = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not is_colab:\n",
    "    # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "USE_ADABINS = True\n",
    "\n",
    "if is_colab:\n",
    "    if not google_drive:\n",
    "        root_path = f'/content'\n",
    "        model_path = '/content/models' \n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "    model_path = f'{root_path}/models'\n",
    "\n",
    "multipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "print(multipip_res)\n",
    "\n",
    "if is_colab:\n",
    "    subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "try:\n",
    "    from CLIP import clip\n",
    "except:\n",
    "    if not os.path.exists(\"CLIP\"):\n",
    "        gitclone(\"https://github.com/openai/CLIP\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
    "\n",
    "try:\n",
    "    import open_clip\n",
    "except:\n",
    "    if not os.path.exists(\"open_clip/src\"):\n",
    "        gitclone(\"https://github.com/mlfoundations/open_clip.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/open_clip/src')\n",
    "    import open_clip\n",
    "\n",
    "try:\n",
    "    from guided_diffusion.script_util import create_model_and_diffusion\n",
    "except:\n",
    "    if not os.path.exists(\"guided-diffusion\"):\n",
    "        gitclone(\"https://github.com/tanukilte/guided-diffusion\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n",
    "\n",
    "try:\n",
    "    from resize_right import resize\n",
    "except:\n",
    "    if not os.path.exists(\"ResizeRight\"):\n",
    "        gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n",
    "\n",
    "\n",
    "if not os.path.exists(\"DiscoDiffusion\"):\n",
    "    gitclone(\"https://github.com/tanukilte/DiscoDiffusionTanuki.git\")\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import timm\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from glob import glob\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "from CLIP import clip\n",
    "from resize_right import resize\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ipywidgets import Output\n",
    "import hashlib\n",
    "from functools import partial\n",
    "if is_colab:\n",
    "    os.chdir('/content')\n",
    "    from google.colab import files\n",
    "else:\n",
    "    os.chdir(f'{PROJECT_DIR}')\n",
    "from IPython.display import Image as ipyimg\n",
    "from numpy import asarray\n",
    "from einops import rearrange, repeat\n",
    "import torch, torchvision\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device('cuda:0' if (torch.cuda.is_available() and not useCPU) else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "device = DEVICE # At least one of the modules expects this name..\n",
    "\n",
    "if not useCPU:\n",
    "    if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
    "        print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        \n",
    "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "cutout_debug = False\n",
    "padargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "DefFns"
   },
   "outputs": [],
   "source": [
    "#@title 1.5 Define necessary functions\n",
    "\n",
    "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def val_interpolate(x1, y1, x2, y2, x):\n",
    "    #Linear interpolation. Return y between y1 and y2 for the same position x is bettewen x1 and x2 \n",
    "    output = (y1* (x2 - x) + y2 * (x - x1))/(x2 - x1)\n",
    "    if type(y1) == int:\n",
    "        output = round(output) # return the proper type\n",
    "    return(output)\n",
    "\n",
    "def smooth_jazz(schedule):\n",
    "\n",
    "    # Take a list of numbers (i.e. an already-evaluated schedule),\n",
    "    # find the places where the number changes from one to the next, and smooth those transitions\n",
    "    newschedule = schedule.copy() #newschedule = [i for i in schedule]\n",
    "    markers = []\n",
    "    last_num = schedule[0]\n",
    "    # build a list of indicies of where the number changes\n",
    "    for i in range(1, len(schedule)):\n",
    "        current_num = schedule[i]\n",
    "        if current_num != last_num:\n",
    "            markers.append(i)\n",
    "        last_num = current_num\n",
    "    # now smooth out the surrounding numbers for any markers we have\n",
    "    lastindex = 0\n",
    "    for i in range(len(markers)):\n",
    "        for k in range(lastindex, markers[i]):\n",
    "            newschedule[k] = val_interpolate(lastindex, schedule[lastindex], markers[i], schedule[markers[i]], k)\n",
    "        lastindex = markers[i]\n",
    "    return(newschedule)\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from matplotlib import pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "def bernstein_poly(i, n, t):\n",
    "    return comb(n, i) * ( t**i ) * (1 - t)**(n-i)\n",
    "\n",
    "\n",
    "def bezier_curve(points, isInt=False):\n",
    "    nPoints = len(points)\n",
    "    xPoints = np.array([p[0] for p in points])\n",
    "    yPoints = np.array([p[1] for p in points])\n",
    "    curveEnd = 1000 - points[-1][0]\n",
    "    t = np.linspace(0.0, 1.0, 1000 - curveEnd)\n",
    "\n",
    "    polynomial_array = np.array([ bernstein_poly(i, nPoints-1, t) for i in range(0, nPoints)   ])\n",
    "\n",
    "    #xvals = np.dot(xPoints, polynomial_array)\n",
    "    xvals = [i for i in range(1000)]\n",
    "    yvals = np.dot(yPoints, polynomial_array)\n",
    "    if isInt:\n",
    "        yvals = [round(i) for i in yvals]\n",
    "    if curveEnd > 0:\n",
    "        yvals = list(yvals) + [yvals[-1] for i in range(1000 - curveEnd, 1000)]\n",
    "    return xvals, yvals\n",
    "\n",
    "def plot_bezier(points, isInt=False, reg=None):\n",
    "    xpoints = [p[0] for p in points]\n",
    "    ypoints = [p[1] for p in points]\n",
    "    name = [name for name in globals() if globals()[name] is points]\n",
    "    if name is not None:\n",
    "        if len(name) > 0:\n",
    "            plt.title(name[0])\n",
    "    xvals, yvals = bezier_curve(points, isInt=isInt)\n",
    "    plt.plot(xvals, yvals)\n",
    "    plt.plot(xpoints, ypoints, \"ro\")\n",
    "    for nr in range(len(points)):\n",
    "        plt.text(points[nr][0], points[nr][1], nr)\n",
    "    if reg is not None:\n",
    "        x1 = [i for i in range(1000)]\n",
    "        plt.plot(x1, reg, label = \"line 1\")   \n",
    "    \n",
    "def perlin(width, height, scale=10, device=None):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 3):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale, device)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
    "    else:\n",
    "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
    "        out = TF.resize(size=(side_y, side_x), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "\n",
    "    out = ImageOps.autocontrast(out)\n",
    "    return out\n",
    "\n",
    "def regen_perlin():\n",
    "    if perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(batch_size, -1, -1, -1)\n",
    "\n",
    "def read_image_workaround(path):\n",
    "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
    "    this incompatibility to avoid colour inversions.\"\"\"\n",
    "    im_tmp = cv2.imread(path)\n",
    "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.reshape([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.reshape([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.skip_augs = skip_augs\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.15),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "\n",
    "        cutouts = []\n",
    "        for ch in range(self.cutn):\n",
    "            if ch > self.cutn - self.cutn//4:\n",
    "                cutout = input.clone()\n",
    "            else:\n",
    "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
    "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
    "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "\n",
    "            if not self.skip_augs:\n",
    "                cutout = self.augs(cutout)\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "            del cutout\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "class MakeCutoutsDango(nn.Module):\n",
    "    def __init__(self, cut_size,\n",
    "                 Overview=4, \n",
    "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.Overview = Overview\n",
    "        self.InnerCrop = InnerCrop\n",
    "        self.IC_Size_Pow = IC_Size_Pow\n",
    "        self.IC_Grey_P = IC_Grey_P\n",
    "        if args.animation_mode == 'None':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif args.animation_mode == 'Video Input':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.15),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.4),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "            ])\n",
    "          \n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        gray = T.Grayscale(3)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        l_size = max(sideX, sideY)\n",
    "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
    "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
    "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
    "        #MOD more useful cuts\n",
    "        if self.Overview > 0:\n",
    "            for i in range(self.Overview):\n",
    "                cutout = resize(pad_input, out_shape=output_shape)\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(gray(TF.hflip(cutout)))\n",
    "                    else:\n",
    "                        cutouts.append(gray(cutout))\n",
    "                else:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(TF.hflip(cutout))\n",
    "                    else:\n",
    "                        cutouts.append(cutout)\n",
    "\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
    "\n",
    "                              \n",
    "        if self.InnerCrop >0:\n",
    "            for i in range(self.InnerCrop):\n",
    "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
    "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "                offsety = torch.randint(0, sideY - size + 1, ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    cutout = gray(cutout)\n",
    "                cutout = resize(cutout, out_shape=output_shape)\n",
    "                cutouts.append(cutout)\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"/content/cutout_InnerCrop.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
    "        cutouts = torch.cat(cutouts)\n",
    "        if skip_augs is not True:\n",
    "            for i in range(cutouts.shape[0]):\n",
    "                cutouts[i]=self.augs(cutouts[i])\n",
    "        return cutouts\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def symmetry_transformation_fn(x):\n",
    "    if args.use_horizontal_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :, :w//2], torch.flip(x[:, :, :, :w//2], [-1])), -1)\n",
    "        print(\"horizontal symmetry applied\")\n",
    "    if args.use_vertical_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :h//2, :], torch.flip(x[:, :, :h//2, :], [-2])), -2)\n",
    "        print(\"vertical symmetry applied\")\n",
    "    return x\n",
    "\n",
    "def do_run():\n",
    "    seed = args.seed# + batch_num\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if args.init_image in ['','none', 'None', 'NONE']:\n",
    "        init_image = None\n",
    "    else:\n",
    "        init_image = args.init_image\n",
    "        init_scale = args.init_scale\n",
    "        skip_steps = args.skip_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    if args.prompts_series is not None:\n",
    "        prompts = args.prompts_series\n",
    "    else:\n",
    "        prompts = []\n",
    "\n",
    "    \n",
    "    if args.image_prompts_series is not None:\n",
    "        print(args.image_prompts_series)\n",
    "        image_prompts = args.image_prompts_series\n",
    "    else:\n",
    "        image_prompts = []\n",
    "\n",
    "    model_stats = []\n",
    "    for clip_model in clip_models:\n",
    "        cutn = 16\n",
    "        model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n",
    "        model_stat[\"clip_model\"] = clip_model\n",
    "\n",
    "        for prompt in prompts:\n",
    "            txt, weight = parse_prompt(prompt)\n",
    "            txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "\n",
    "            if args.fuzzy_prompt:\n",
    "                for i in range(25):\n",
    "                    model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n",
    "                    model_stat[\"weights\"].append(weight)\n",
    "            else:\n",
    "                model_stat[\"target_embeds\"].append(txt)\n",
    "                model_stat[\"weights\"].append(weight)\n",
    "\n",
    "        if image_prompts:\n",
    "            model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n",
    "            for prompt in image_prompt:\n",
    "                path, weight = parse_prompt(prompt)\n",
    "                img = Image.open(fetch(path)).convert('RGB')\n",
    "                img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
    "                batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "                embed = clip_model.encode_image(normalize(batch)).float()\n",
    "                if fuzzy_prompt:\n",
    "                    for i in range(25):\n",
    "                        model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n",
    "                        weights.extend([weight / cutn] * cutn)\n",
    "                else:\n",
    "                    model_stat[\"target_embeds\"].append(embed)\n",
    "                    model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
    "\n",
    "        model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
    "        model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
    "        if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
    "            raise RuntimeError('The weights must not sum to 0.')\n",
    "        model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
    "        model_stats.append(model_stat)\n",
    "\n",
    "        init = None\n",
    "        if init_image is not None:\n",
    "            init = Image.open(fetch(init_image)).convert('RGB')\n",
    "            init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
    "            init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        #init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, y=None):\n",
    "        with torch.enable_grad():\n",
    "            x_is_NaN = False\n",
    "            x = x.detach().requires_grad_()\n",
    "            n = x.shape[0]\n",
    "            if use_secondary_model is True:\n",
    "                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
    "                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "            else:\n",
    "                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "                \n",
    "            #MOD this whole chunk and spliting off cuts outer/inner:\n",
    "            normalizeGuideToRes = width_height[0] * width_height[1] / 425984.0 #Normalize to 832,512 res\n",
    "                \n",
    "            for model_stat in model_stats:\n",
    "                t_int = int(t.item())+1 #errors on last step without +1, need to find source\n",
    "                #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
    "                try:\n",
    "                    input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n",
    "                except:\n",
    "                    input_resolution=224\n",
    "                for i in range(args.cutn_batches[1000-t_int]):\n",
    "                    cuts = MakeCutoutsDango(input_resolution,\n",
    "                            Overview= args.cut_overview[1000-t_int], \n",
    "                            InnerCrop =args.cut_innercut[1000-t_int],\n",
    "                            IC_Size_Pow=args.cut_ic_pow[1000-t_int],\n",
    "                            IC_Grey_P = args.cut_icgray_p[1000-t_int]\n",
    "                            )\n",
    "                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
    "                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
    "                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
    "                    dists = dists.view([args.cut_overview[1000-t_int] + args.cut_innercut[1000-t_int], n, -1])\n",
    "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
    "                    #loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n",
    "                    x_in_grad += torch.autograd.grad(losses.sum() * args.clip_guidance_scale[1000-t_int], x_in)[0] #/ args.cutn_batches[1000-t_int]\n",
    "            tv_losses = tv_loss(x_in)\n",
    "            if use_secondary_model is True:\n",
    "                range_losses = range_loss(out)\n",
    "            else:\n",
    "                range_losses = range_loss(out['pred_xstart'])\n",
    "            sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
    "            loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "            #if init is not None and init_scale:\n",
    "                #init_losses = lpips_model(x_in, init)\n",
    "                #loss = loss + init_losses.sum() * init_scale\n",
    "            x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "            if torch.isnan(x_in_grad).any()==False:\n",
    "                grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "            else:\n",
    "                # print(\"NaN'd\")\n",
    "                x_is_NaN = True\n",
    "                grad = torch.zeros_like(x)\n",
    "        if args.clamp_grad and x_is_NaN == False:\n",
    "            magnitude = grad.square().mean().sqrt()\n",
    "            grad = grad * magnitude.clamp(max=args.clamp_max[1000 - t_int]) / magnitude  #min=-0.02, min=-clamp_max, \n",
    "        #MOD:    \n",
    "        #Static thresholding: We refer to elementwise clipping the x-prediction to [1, 1] as static thresholding. This method was in fact used but not emphasized in previous work [28 ], and to our knowledge its importance has not been investigated in the context of guided sampling. We discover that static thresholding is essential to sampling with large guidance weights and prevents generation of blank images. Nonetheless, static thresholding still results in over-saturated and less detailed images as the guidance weight further increases.\n",
    "        #made dynamic clamp instead of regular clamp\n",
    "        return grad.div(max(grad.max(), 0.0 - grad.min(), 1.0))\n",
    "\n",
    "    if args.diffusion_sampling_mode == 'ddim':\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.plms_sample_loop_progressive\n",
    "\n",
    "\n",
    "    image_display = Output()\n",
    "    for i in range(args.n_batches):\n",
    "        display.clear_output(wait=True)\n",
    "        batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n",
    "        batchBar.n = i\n",
    "        batchBar.refresh()\n",
    "        print('')\n",
    "        display.display(image_display)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        cur_t = diffusion.num_timesteps - skip_steps - 1\n",
    "        total_steps = cur_t\n",
    "\n",
    "        if perlin_init:\n",
    "            init = regen_perlin()\n",
    "\n",
    "        if args.diffusion_sampling_mode == 'ddim':\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                eta=args.eta,\n",
    "                dynamicThreshold=args.dynamicThreshold,\n",
    "                transformation_fn=symmetry_transformation_fn,\n",
    "                transformation_percent=args.transformation_percent\n",
    "            )\n",
    "        else:\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                order=2,\n",
    "        )\n",
    "\n",
    "        # with run_display:\n",
    "        # display.clear_output(wait=True)\n",
    "        for j, sample in enumerate(samples):    \n",
    "            cur_t -= 1\n",
    "            intermediateStep = False\n",
    "            if args.steps_per_checkpoint is not None:\n",
    "                if j % steps_per_checkpoint == 0 and j > 0:\n",
    "                    intermediateStep = True\n",
    "            elif j in args.intermediate_saves:\n",
    "                intermediateStep = True\n",
    "            with image_display:\n",
    "                if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
    "                    for k, image in enumerate(sample['pred_xstart']):\n",
    "                        # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                        current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
    "                        percent = math.ceil(j/total_steps*100)\n",
    "                        if args.n_batches > 0:\n",
    "                            #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n",
    "                            if cur_t == -1 and args.intermediates_in_subfolder is True:\n",
    "                                save_num = f'{frame_num:04}' if args.animation_mode != \"None\" else i\n",
    "                                filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n",
    "                            else:\n",
    "                                #If we're working with percentages, append it\n",
    "                                if args.steps_per_checkpoint is not None:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n",
    "                                # Or else, iIf we're working with specific steps, append those\n",
    "                                else:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n",
    "                        image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
    "                        if j % args.display_rate == 0 or cur_t == -1:\n",
    "                            image.save('progress.png')\n",
    "                            display.clear_output(wait=True)\n",
    "                            display.display(display.Image('progress.png'))\n",
    "                        if args.steps_per_checkpoint is not None:\n",
    "                            if j % args.steps_per_checkpoint == 0 and j > 0:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        else:\n",
    "                            if j in args.intermediate_saves:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        if cur_t == -1:\n",
    "                            save_settings()\n",
    "                            image.save(f'{batchFolder}/{filename}')\n",
    "                    \n",
    "      #plt.plot(np.array(loss_values), 'r')\n",
    "\n",
    "def save_settings():\n",
    "    setting_list = {\n",
    "        'text_prompts': text_prompts,\n",
    "        'image_prompts': image_prompts,\n",
    "        'clip_guidance_scale': clip_guidance_scale,\n",
    "        'tv_scale': tv_scale,\n",
    "        'range_scale': range_scale,\n",
    "        'sat_scale': sat_scale,\n",
    "        'cutn_batches': cutn_batches,\n",
    "        'init_image': init_image,\n",
    "        'init_scale': init_scale,\n",
    "        'skip_steps': skip_steps,\n",
    "        'perlin_init': perlin_init,\n",
    "        'perlin_mode': perlin_mode,\n",
    "        'skip_augs': skip_augs,\n",
    "        'randomize_class': randomize_class,\n",
    "        'clip_denoised': clip_denoised,\n",
    "        'clamp_grad': clamp_grad,\n",
    "        'clamp_max': clamp_max,\n",
    "        'seed': seed,\n",
    "        'fuzzy_prompt': fuzzy_prompt,\n",
    "        'rand_mag': rand_mag,\n",
    "        'eta': eta,\n",
    "        \"dynamicThreshold\": dynamicThreshold,\n",
    "        'width': width_height[0],\n",
    "        'height': width_height[1],\n",
    "        'diffusion_model': diffusion_model,\n",
    "        'use_secondary_model': use_secondary_model,\n",
    "        'steps': steps,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "        'ViTB32': ViTB32,\n",
    "        'ViTB16': ViTB16,\n",
    "        'ViTL14': ViTL14,\n",
    "        'ViTL14_336px': ViTL14_336px,\n",
    "        'RN101': RN101,\n",
    "        'RN50': RN50,\n",
    "        'RN50x4': RN50x4,\n",
    "        'RN50x16': RN50x16,\n",
    "        'RN50x64': RN50x64,\n",
    "        'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "        'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "        'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "        'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "        'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "        'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "        'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "        'RN50_yffcc15m': RN50_yffcc15m,\n",
    "        'RN50_cc12m': RN50_cc12m,\n",
    "        'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "        'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "        'RN101_yfcc15m': RN101_yfcc15m,\n",
    "        'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "        'cut_overview': str(cut_overview),\n",
    "        'cut_innercut': str(cut_innercut),\n",
    "        'cut_ic_pow': str(cut_ic_pow),\n",
    "        'cut_icgray_p': str(cut_icgray_p),\n",
    "        'sampling_mode': sampling_mode,\n",
    "        'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "        'use_vertical_symmetry':use_vertical_symmetry,\n",
    "        'transformation_percent':transformation_percent,\n",
    "    }\n",
    "    # print('Settings:', setting_list)\n",
    "    with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "        json.dump(setting_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "DefSecModel"
   },
   "outputs": [],
   "source": [
    "#@title 1.6 Define the secondary diffusion model\n",
    "\n",
    "def append_dims(x, n):\n",
    "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
    "\n",
    "\n",
    "def expand_to_planes(x, shape):\n",
    "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
    "\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
    "\n",
    "\n",
    "def t_to_alpha_sigma(t):\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionOutput:\n",
    "    v: torch.Tensor\n",
    "    pred: torch.Tensor\n",
    "    eps: torch.Tensor\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, c),\n",
    "            ConvBlock(c, c),\n",
    "            SkipBlock([\n",
    "                nn.AvgPool2d(2),\n",
    "                ConvBlock(c, c * 2),\n",
    "                ConvBlock(c * 2, c * 2),\n",
    "                SkipBlock([\n",
    "                    nn.AvgPool2d(2),\n",
    "                    ConvBlock(c * 2, c * 4),\n",
    "                    ConvBlock(c * 4, c * 4),\n",
    "                    SkipBlock([\n",
    "                        nn.AvgPool2d(2),\n",
    "                        ConvBlock(c * 4, c * 8),\n",
    "                        ConvBlock(c * 8, c * 4),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                    ]),\n",
    "                    ConvBlock(c * 8, c * 4),\n",
    "                    ConvBlock(c * 4, c * 2),\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                ]),\n",
    "                ConvBlock(c * 4, c * 2),\n",
    "                ConvBlock(c * 2, c),\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            ]),\n",
    "            ConvBlock(c * 2, c),\n",
    "            nn.Conv2d(c, 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.down = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, cs[0]),\n",
    "            ConvBlock(cs[0], cs[0]),\n",
    "            SkipBlock([\n",
    "                self.down,\n",
    "                ConvBlock(cs[0], cs[1]),\n",
    "                ConvBlock(cs[1], cs[1]),\n",
    "                SkipBlock([\n",
    "                    self.down,\n",
    "                    ConvBlock(cs[1], cs[2]),\n",
    "                    ConvBlock(cs[2], cs[2]),\n",
    "                    SkipBlock([\n",
    "                        self.down,\n",
    "                        ConvBlock(cs[2], cs[3]),\n",
    "                        ConvBlock(cs[3], cs[3]),\n",
    "                        SkipBlock([\n",
    "                            self.down,\n",
    "                            ConvBlock(cs[3], cs[4]),\n",
    "                            ConvBlock(cs[4], cs[4]),\n",
    "                            SkipBlock([\n",
    "                                self.down,\n",
    "                                ConvBlock(cs[4], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[4]),\n",
    "                                self.up,\n",
    "                            ]),\n",
    "                            ConvBlock(cs[4] * 2, cs[4]),\n",
    "                            ConvBlock(cs[4], cs[3]),\n",
    "                            self.up,\n",
    "                        ]),\n",
    "                        ConvBlock(cs[3] * 2, cs[3]),\n",
    "                        ConvBlock(cs[3], cs[2]),\n",
    "                        self.up,\n",
    "                    ]),\n",
    "                    ConvBlock(cs[2] * 2, cs[2]),\n",
    "                    ConvBlock(cs[2], cs[1]),\n",
    "                    self.up,\n",
    "                ]),\n",
    "                ConvBlock(cs[1] * 2, cs[1]),\n",
    "                ConvBlock(cs[1], cs[0]),\n",
    "                self.up,\n",
    "            ]),\n",
    "            ConvBlock(cs[0] * 2, cs[0]),\n",
    "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffClipSetTop"
   },
   "source": [
    "# 2. Diffusion and CLIP model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ModelSettings"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512x512_diffusion_uncond_finetune_008100 already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "secondary already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\Zhenya\\.conda\\envs\\DiscoDiffusionTanuki\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#@markdown ####**Models Settings (note: For pixel art, the best is pixelartdiffusion_expanded):**\n",
    "diffusion_model = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\", \"portrait_generator_v001\", \"pixelartdiffusion_expanded\", \"pixel_art_diffusion_hard_256\", \"pixel_art_diffusion_soft_256\", \"pixelartdiffusion4k\", \"watercolordiffusion_2\", \"watercolordiffusion\", \"PulpSciFiDiffusion\", \"custom\"]\n",
    "\n",
    "use_secondary_model = True #@param {type: 'boolean'}\n",
    "diffusion_sampling_mode = 'ddim' #@param ['plms','ddim']\n",
    "#@markdown #####**Custom model:**\n",
    "custom_path = '/content/drive/MyDrive/deep_learning/ddpm/ema_0.9999_058000.pt'#@param {type: 'string'}\n",
    "\n",
    "#@markdown #####**CLIP settings:**\n",
    "use_checkpoint = True #@param {type: 'boolean'}\n",
    "ViTB32 = True #@param{type:\"boolean\"}\n",
    "ViTB16 = True #@param{type:\"boolean\"}\n",
    "ViTL14 = True #@param{type:\"boolean\"}\n",
    "ViTL14_336px = False #@param{type:\"boolean\"}\n",
    "RN101 = False #@param{type:\"boolean\"}\n",
    "RN50 = False #@param{type:\"boolean\"}\n",
    "RN50x4 = False #@param{type:\"boolean\"}\n",
    "RN50x16 = True #@param{type:\"boolean\"}\n",
    "RN50x64 = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown #####**OpenCLIP settings:**\n",
    "ViTB32_laion2b_e16 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_32 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "RN50_yffcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_cc12m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_cc12m = False #@param{type:\"boolean\"}\n",
    "RN101_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN101_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown If you're having issues with model downloads, check this to compare SHA's:\n",
    "check_model_SHA = False #@param{type:\"boolean\"}\n",
    "\n",
    "diff_model_map = {\n",
    "    '256x256_diffusion_uncond': { 'downloaded': False, 'sha': 'a37c32fffd316cd494cf3f35b339936debdc1576dad13fe57c42399a5dbc78b1', 'uri_list': ['https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt', 'https://www.dropbox.com/s/9tqnqo930mpnpcn/256x256_diffusion_uncond.pt'] },\n",
    "    '512x512_diffusion_uncond_finetune_008100': { 'downloaded': False, 'sha': '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648', 'uri_list': ['https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt', 'https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt'] },\n",
    "    'portrait_generator_v001': { 'downloaded': False, 'sha': 'b7e8c747af880d4480b6707006f1ace000b058dd0eac5bb13558ba3752d9b5b9', 'uri_list': ['https://huggingface.co/felipe3dartist/portrait_generator_v001/resolve/main/portrait_generator_v001_ema_0.9999_1MM.pt'] },\n",
    "    'pixelartdiffusion_expanded': { 'downloaded': False, 'sha': 'a73b40556634034bf43b5a716b531b46fb1ab890634d854f5bcbbef56838739a', 'uri_list': ['https://huggingface.co/KaliYuga/PADexpanded/resolve/main/PADexpanded.pt'] },\n",
    "    'pixel_art_diffusion_hard_256': { 'downloaded': False, 'sha': 'be4a9de943ec06eef32c65a1008c60ad017723a4d35dc13169c66bb322234161', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_hard_256/resolve/main/pixel_art_diffusion_hard_256.pt'] },\n",
    "    'pixel_art_diffusion_soft_256': { 'downloaded': False, 'sha': 'd321590e46b679bf6def1f1914b47c89e762c76f19ab3e3392c8ca07c791039c', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_soft_256/resolve/main/pixel_art_diffusion_soft_256.pt'] },\n",
    "    'pixelartdiffusion4k': { 'downloaded': False, 'sha': 'a1ba4f13f6dabb72b1064f15d8ae504d98d6192ad343572cc416deda7cccac30', 'uri_list': ['https://huggingface.co/KaliYuga/pixelartdiffusion4k/resolve/main/pixelartdiffusion4k.pt'] },\n",
    "    'watercolordiffusion_2': { 'downloaded': False, 'sha': '49c281b6092c61c49b0f1f8da93af9b94be7e0c20c71e662e2aa26fee0e4b1a9', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion_2/resolve/main/watercolordiffusion_2.pt'] },\n",
    "    'watercolordiffusion': { 'downloaded': False, 'sha': 'a3e6522f0c8f278f90788298d66383b11ac763dd5e0d62f8252c962c23950bd6', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion/resolve/main/watercolordiffusion.pt'] },\n",
    "    'PulpSciFiDiffusion': { 'downloaded': False, 'sha': 'b79e62613b9f50b8a3173e5f61f0320c7dbb16efad42a92ec94d014f6e17337f', 'uri_list': ['https://huggingface.co/KaliYuga/PulpSciFiDiffusion/resolve/main/PulpSciFiDiffusion.pt'] },\n",
    "    'secondary': { 'downloaded': False, 'sha': '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a', 'uri_list': ['https://the-eye.eu/public/AI/models/v-diffusion/secondary_model_imagenet_2.pth', 'https://ipfs.pollinations.ai/ipfs/bafybeibaawhhk7fhyhvmm7x24zwwkeuocuizbqbcg5nqx64jq42j75rdiy/secondary_model_imagenet_2.pth'] },\n",
    "}\n",
    "\n",
    "kaliyuga_pixel_art_model_names = ['pixelartdiffusion_expanded', 'pixel_art_diffusion_hard_256', 'pixel_art_diffusion_soft_256', 'pixelartdiffusion4k', 'PulpSciFiDiffusion']\n",
    "kaliyuga_watercolor_model_names = ['watercolordiffusion', 'watercolordiffusion_2']\n",
    "kaliyuga_pulpscifi_model_names = ['PulpSciFiDiffusion']\n",
    "diffusion_models_256x256_list = ['256x256_diffusion_uncond'] + kaliyuga_pixel_art_model_names + kaliyuga_watercolor_model_names + kaliyuga_pulpscifi_model_names\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_model_filename(diffusion_model_name):\n",
    "    model_uri = diff_model_map[diffusion_model_name]['uri_list'][0]\n",
    "    model_filename = os.path.basename(urlparse(model_uri).path)\n",
    "    return model_filename\n",
    "\n",
    "\n",
    "def download_model(diffusion_model_name, uri_index=0):\n",
    "    if diffusion_model_name != 'custom':\n",
    "        model_filename = get_model_filename(diffusion_model_name)\n",
    "        model_local_path = os.path.join(model_path, model_filename)\n",
    "        if os.path.exists(model_local_path) and check_model_SHA:\n",
    "            print(f'Checking {diffusion_model_name} File')\n",
    "            with open(model_local_path, \"rb\") as f:\n",
    "                bytes = f.read() \n",
    "                hash = hashlib.sha256(bytes).hexdigest()\n",
    "            if hash == diff_model_map[diffusion_model_name]['sha']:\n",
    "                print(f'{diffusion_model_name} SHA matches')\n",
    "                diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "            else:\n",
    "                print(f\"{diffusion_model_name} SHA doesn't match. Will redownload it.\")\n",
    "        elif os.path.exists(model_local_path) and not check_model_SHA or diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            print(f'{diffusion_model_name} already downloaded. If the file is corrupt, enable check_model_SHA.')\n",
    "            diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "\n",
    "        if not diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            for model_uri in diff_model_map[diffusion_model_name]['uri_list']:\n",
    "                fetchsave(model_uri, model_local_path)\n",
    "                if os.path.exists(model_local_path):\n",
    "                    diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "                    return\n",
    "                else:\n",
    "                    print(f'{diffusion_model_name} model download from {model_uri} failed. Will try any fallback uri.')\n",
    "            print(f'{diffusion_model_name} download failed.')\n",
    "\n",
    "\n",
    "# Download the diffusion model(s)\n",
    "download_model(diffusion_model)\n",
    "if use_secondary_model:\n",
    "    download_model('secondary')\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == '256x256_diffusion_uncond':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 256,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == 'portrait_generator_v001':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000,\n",
    "        'rescale_timesteps': True,\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 128,\n",
    "        'num_heads': 4,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "else:  # E.g. A model finetuned by KaliYuga\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "    \n",
    "if diffusion_model == 'custom':\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "\n",
    "model_default = model_config['image_size']\n",
    "\n",
    "if use_secondary_model:\n",
    "    secondary_model = SecondaryDiffusionImageNet2()\n",
    "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "    secondary_model.eval().requires_grad_(False).to(device)\n",
    "\n",
    "clip_models = []\n",
    "if ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion2b_e16: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion2b_e16').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_32: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if RN50_yffcc15m: clip_models.append(open_clip.create_model('RN50', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_cc12m: clip_models.append(open_clip.create_model('RN50', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_cc12m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN101_yfcc15m: clip_models.append(open_clip.create_model('RN101', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN101_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN101-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SettingsTop"
   },
   "source": [
    "# 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BasicSettings"
   },
   "outputs": [],
   "source": [
    "#@markdown ####**Basic Settings:**\n",
    "batch_name = 'DDT' #@param{type: 'string'}\n",
    "display_rate = 20 #@param{type: 'number'}\n",
    "n_batches = 5 #@param{type: 'number'}\n",
    "steps = 250 #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\n",
    "width_height_for_512x512_models = [960, 576] #@param{type: 'raw'}\n",
    "clip_guidance_scale = \"[15000]*100+[16000]*400+[12000]*499+[10000]*1\" #@param{type: 'string'}\n",
    "tv_scale = 0#@param{type: 'number'}\n",
    "range_scale = 150#@param{type: 'number'}\n",
    "sat_scale = 0#@param{type: 'number'}\n",
    "cutn_batches = \"[2]*200+[2]*200+[4]*200+[4]*200+[4]*200\"#@param{type: 'string'}\n",
    "skip_augs = False#@param{type: 'boolean'}\n",
    "dynamicThreshold = [[0,0.036],[500,0.088],[1000,0.088]]\n",
    "#@markdown ####**Image dimensions to be used for 256x256 models (e.g. pixelart models):**\n",
    "width_height_for_256x256_models = [512, 448] #@param{type: 'raw'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Init Image Settings:**\n",
    "init_image = None #@param{type: 'string'}\n",
    "init_scale = 1000 #@param{type: 'integer'}\n",
    "skip_steps = 0 #@param{type: 'integer'}\n",
    "#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n",
    "\n",
    "width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "\n",
    "#Get corrected sizes\n",
    "side_x = (width_height[0]//64)*64;\n",
    "side_y = (width_height[1]//64)*64;\n",
    "if side_x != width_height[0] or side_y != width_height[1]:\n",
    "    print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "\n",
    "#Make folder for batch\n",
    "batchFolder = f'{outDirPath}/{batch_name}'\n",
    "createPath(batchFolder)\n",
    "#@markdown ####**Saving:**\n",
    "\n",
    "intermediate_saves = [50, 100, 150, 200]#@param{type: 'raw'}\n",
    "intermediates_in_subfolder = True #@param{type: 'boolean'}\n",
    "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
    "\n",
    "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
    "\n",
    "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
    "\n",
    "\n",
    "if type(intermediate_saves) is not list:\n",
    "    if intermediate_saves:\n",
    "        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "        print(f'Will save every {steps_per_checkpoint} steps')\n",
    "    else:\n",
    "        steps_per_checkpoint = steps+10\n",
    "else:\n",
    "    steps_per_checkpoint = None\n",
    "\n",
    "if intermediate_saves and intermediates_in_subfolder is True:\n",
    "    partialFolder = f'{batchFolder}/partials'\n",
    "    createPath(partialFolder)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Advanced Settings:**\n",
    "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
    "\n",
    "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
    "\n",
    "perlin_init = True  #@param{type: 'boolean'}\n",
    "perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\n",
    "set_seed = 'random_seed' #@param{type: 'string'}\n",
    "eta = \"[0.0]*200+[0.001]*799+[0.36]*1\"#@param{type: 'string'}\n",
    "clamp_grad = True #@param{type: 'boolean'}\n",
    "clamp_max = \"[0.064]*999+[0.088]*1\" #@param{type: 'string'}\n",
    "frames_skip_steps = \"0%\"#@param{type: 'string'}\n",
    "\n",
    "### EXTRA ADVANCED SETTINGS:\n",
    "randomize_class = True\n",
    "clip_denoised = False\n",
    "fuzzy_prompt = False\n",
    "rand_mag = 0.05\n",
    "sampling_mode = \"ddim\"#@param{type: 'string'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Cutn Scheduling:**\n",
    "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
    "\n",
    "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
    "\n",
    "cut_overview = \"[6]*200+[5]*200+[2]*200+[0]*200+[0]*200\" #@param {type: 'string'}       \n",
    "cut_innercut = \"[0]*200+[1]*200+[4]*200+[6]*200+[6]*200\" #@param {type: 'string'}  \n",
    "cut_ic_pow = \"[1]*200+[4]*200+[8]*200+[16]*200+[32]*100+[88]*100\" #@param {type: 'string'}  \n",
    "cut_icgray_p = \"[0.21]*150+[0.2]*350+[0]*100+[0]*100+[0]*300\" #@param {type: 'string'}\n",
    "\n",
    "#@markdown KaliYuga model settings. Refer to [cut_ic_pow](https://ezcharts.miraheze.org/wiki/Category:Cut_ic_pow) as a guide. Values between 1 and 100 all work.\n",
    "pad_or_pulp_cut_overview = \"[15]*100+[15]*100+[12]*100+[12]*100+[6]*100+[4]*100+[2]*200+[0]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_innercut = \"[1]*100+[1]*100+[4]*100+[4]*100+[8]*100+[8]*100+[10]*200+[10]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_icgray_p = \"[0.87]*100+[0.78]*50+[0.73]*50+[0.64]*60+[0.56]*40+[0.50]*50+[0.33]*100+[0.19]*150+[0]*400\" #@param {type: 'string'}\n",
    "\n",
    "watercolor_cut_overview = \"[14]*200+[12]*200+[4]*400+[0]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_innercut = \"[2]*200+[4]*200+[12]*400+[12]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "watercolor_cut_icgray_p = \"[0.7]*100+[0.6]*100+[0.45]*100+[0.3]*100+[0]*600\" #@param {type: 'string'}\n",
    "\n",
    "if (diffusion_model in kaliyuga_pixel_art_model_names) or (diffusion_model in kaliyuga_pulpscifi_model_names):\n",
    "    cut_overview = pad_or_pulp_cut_overview\n",
    "    cut_innercut = pad_or_pulp_cut_innercut\n",
    "    cut_ic_pow = pad_or_pulp_cut_ic_pow\n",
    "    cut_icgray_p = pad_or_pulp_cut_icgray_p\n",
    "elif diffusion_model in kaliyuga_watercolor_model_names:\n",
    "    cut_overview = watercolor_cut_overview\n",
    "    cut_innercut = watercolor_cut_innercut\n",
    "    cut_ic_pow = watercolor_cut_ic_pow\n",
    "    cut_icgray_p = watercolor_cut_icgray_p\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Transformation Settings:**\n",
    "use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
    "use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
    "transformation_percent = [0.09] #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Local Settings\n",
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    #Modifying settings for local notebook easier here\n",
    "    batch_name = 'DDT'\n",
    "    n_batches = 5\n",
    "    width_height_for_512x512_models = [960,576]\n",
    "    steps = 500\n",
    "    set_seed = 'random_seed'\n",
    "    display_rate = 20\n",
    "    intermediate_saves = [50, 100, 150, 200, 250, 300, 350, 400, 450]\n",
    "    intermediates_in_subfolder = True\n",
    "    clip_guidance_scale = [[0,16000],[500,16000],[1000,12000]]\n",
    "    frames_skip_steps = \"0%\"\n",
    "    tv_scale = 15000\n",
    "    range_scale = 40000\n",
    "    sat_scale = 40000\n",
    "    cutn_batches = [[0,2],[300,4],[1000,6]]\n",
    "    cut_overview = [[0,6],[100,3],[1000,0]]      \n",
    "    cut_innercut = [[0,0],[100,3],[1000,6]]\n",
    "    cut_ic_pow = [[0,0],[500,0.0],[1000,24]]\n",
    "    cut_icgray_p = [[0,0.25],[1000,0.25],[400,0]]\n",
    "    clamp_grad = True\n",
    "    clamp_max = [[0,0.048],[500,0.12],[1000,0.064]]\n",
    "    eta = [[0,0.0],[0,0.64],[1000,0.88]]\n",
    "    perlin_init = True\n",
    "    perlin_mode = 'mixed'\n",
    "    skip_augs = False\n",
    "    use_vertical_symmetry = False\n",
    "    use_horizontal_symmetry = False\n",
    "    transformation_percent = [0.09]\n",
    "    dynamicThreshold=[[0,0.12],[0,1.1],[888,1.2],[1000,0.88]]\n",
    "\n",
    "    #Settings cleanup\n",
    "    width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "    side_x = (width_height[0]//64)*64;\n",
    "    side_y = (width_height[1]//64)*64;\n",
    "    if side_x != width_height[0] or side_y != width_height[1]:\n",
    "        print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "    batchFolder = f'{outDirPath}/{batch_name}'\n",
    "    createPath(batchFolder)\n",
    "    if type(intermediate_saves) is not list:\n",
    "        if intermediate_saves:\n",
    "            steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "            steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "            print(f'Will save every {steps_per_checkpoint} steps')\n",
    "        else:\n",
    "            steps_per_checkpoint = steps+10\n",
    "    else:\n",
    "        steps_per_checkpoint = None\n",
    "    if intermediate_saves and intermediates_in_subfolder is True:\n",
    "        partialFolder = f'{batchFolder}/partials'\n",
    "        createPath(partialFolder)\n",
    "finally:\n",
    "    pass\n",
    "    plt.rcParams[\"figure.figsize\"]=16,16      \n",
    "    plt.subplot(5, 2, 1)\n",
    "    plot_bezier(clip_guidance_scale, True)\n",
    "    plt.subplot(5, 2, 2)\n",
    "    plot_bezier(cutn_batches, True)\n",
    "    plt.subplot(5, 2, 3)\n",
    "    plot_bezier(cut_overview, True)\n",
    "    plt.subplot(5, 2, 4)\n",
    "    plot_bezier(cut_innercut, True)\n",
    "    plt.subplot(5, 2, 5)\n",
    "    plot_bezier(cut_ic_pow)\n",
    "    plt.subplot(5, 2, 6)\n",
    "    plot_bezier(clamp_max)\n",
    "    plt.subplot(5, 2, 7)\n",
    "    plot_bezier(cut_icgray_p)\n",
    "    plt.subplot(5, 2, 8)\n",
    "    plot_bezier(eta)\n",
    "    plt.subplot(5, 2, 9)\n",
    "    plot_bezier(dynamicThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PromptsTop"
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Prompts"
   },
   "outputs": [],
   "source": [
    "# Note: If using a pixelart diffusion model, try adding \"#pixelart\" to the end of the prompt for a stronger effect. It'll tend to work a lot better!\n",
    "text_prompts = {\n",
    "    0: [\"Ancient Pyramid of the Dragon Gods, by federico pelat and greg rutkowski, a beautifully ultradetailed painting of an ancient Egyptian pyramid that glows with a gentle noble aura with statues and monuments to dragon gods, trending on artstation, 4k, ultrawide lens:3\",\n",
    "            \"people, dof, blur:-1\",\n",
    "       ],\n",
    "}\n",
    "\n",
    "image_prompts = {\n",
    "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffuseTop"
   },
   "source": [
    "# 4. Diffuse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoTheRun"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215d24f803e44f3896a6c4771b5ecf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf060dd73904f7688460f752af8ff6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8504b095da3f41b6938b45469270eb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Do the Run!\n",
    "#@markdown `n_batches` ignored with animation modes.\n",
    "\n",
    "#Update Model Settings\n",
    "timestep_respacing = f'ddim{steps}'\n",
    "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
    "model_config.update({\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "})\n",
    "\n",
    "batch_size = 1 \n",
    "\n",
    "def move_files(start_num, end_num, old_folder, new_folder):\n",
    "    for i in range(start_num, end_num):\n",
    "        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "resume_run = False #@param{type: 'boolean'}\n",
    "run_to_resume = 'latest' #@param{type: 'string'}\n",
    "resume_from_frame = 'latest' #@param{type: 'string'}\n",
    "retain_overwritten_frames = False #@param{type: 'boolean'}\n",
    "if retain_overwritten_frames:\n",
    "    retainFolder = f'{batchFolder}/retained'\n",
    "    createPath(retainFolder)\n",
    "\n",
    "\n",
    "skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
    "calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
    "\n",
    "if steps <= calc_frames_skip_steps:\n",
    "    sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
    "\n",
    "start_frame = 0\n",
    "batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
    "while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\"):\n",
    "    batchNum += 1\n",
    "\n",
    "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
    "\n",
    "if set_seed == 'random_seed':\n",
    "    random.seed()\n",
    "    seed = random.randint(0, 2**32)\n",
    "    # print(f'Using seed: {seed}')\n",
    "else:\n",
    "    seed = int(set_seed)\n",
    "\n",
    "    \n",
    "args = {\n",
    "    'batchNum': batchNum,\n",
    "    'prompts_series': text_prompts[0] if text_prompts else None,\n",
    "    'image_prompts_series':image_prompts[0] if image_prompts else None,\n",
    "    'seed': seed,\n",
    "    'display_rate':display_rate,\n",
    "    'n_batches':n_batches,\n",
    "    'batch_size':batch_size,\n",
    "    'batch_name': batch_name,\n",
    "    'steps': steps,\n",
    "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "    'width_height': width_height,\n",
    "    'clip_guidance_scale': bezier_curve(clip_guidance_scale, True)[1],\n",
    "    'tv_scale': tv_scale,\n",
    "    'range_scale': range_scale,\n",
    "    'sat_scale': sat_scale,\n",
    "    'cutn_batches': bezier_curve(cutn_batches, True)[1],\n",
    "    'init_image': init_image,\n",
    "    'init_scale': init_scale,\n",
    "    'skip_steps': skip_steps,\n",
    "    'side_x': side_x,\n",
    "    'side_y': side_y,\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'skip_step_ratio': skip_step_ratio,\n",
    "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
    "    'text_prompts': text_prompts,\n",
    "    'image_prompts': image_prompts,\n",
    "    'cut_overview': bezier_curve(cut_overview, True)[1],\n",
    "    'cut_innercut': bezier_curve(cut_innercut, True)[1],\n",
    "    'cut_ic_pow': bezier_curve(cut_ic_pow)[1],\n",
    "    'cut_icgray_p': bezier_curve(cut_icgray_p)[1],\n",
    "    'intermediate_saves': intermediate_saves,\n",
    "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
    "    'steps_per_checkpoint': steps_per_checkpoint,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'set_seed': set_seed,\n",
    "    'eta': bezier_curve(eta)[1],\n",
    "    \"dynamicThreshold\": bezier_curve(dynamicThreshold)[1],\n",
    "    'clamp_grad': clamp_grad,\n",
    "    'clamp_max': bezier_curve(clamp_max)[1],\n",
    "    'skip_augs': skip_augs,\n",
    "    'randomize_class': randomize_class,\n",
    "    'clip_denoised': clip_denoised,\n",
    "    'fuzzy_prompt': fuzzy_prompt,\n",
    "    'rand_mag': rand_mag,\n",
    "    'use_vertical_symmetry': use_vertical_symmetry,\n",
    "    'use_horizontal_symmetry': use_horizontal_symmetry,\n",
    "    'transformation_percent': transformation_percent,\n",
    "    'animation_mode': 'None',\n",
    "}\n",
    "\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "print('Prepping model...')\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "if diffusion_model == 'custom':\n",
    "    model.load_state_dict(torch.load(custom_path, map_location='cpu'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    do_run()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print('Seed used:', seed)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BezierCurve:\n",
    "    def __init__(self, list_of_points):\n",
    "        self.list_of_points = list_of_points\n",
    "        self.degree = len(list_of_points) - 1\n",
    "\n",
    "    def basis_function(self, t) :\n",
    "\n",
    "        output_values = []\n",
    "        for i in range(len(self.list_of_points)):\n",
    "            output_values.append(\n",
    "                comb(self.degree, i) * ((1 - t) ** (self.degree - i)) * (t**i)\n",
    "            )\n",
    "        return output_values\n",
    "\n",
    "    def bezier_curve_function(self, t):\n",
    "\n",
    "        basis_function = self.basis_function(t)\n",
    "        x = 0.0\n",
    "        y = 0.0\n",
    "        for i in range(len(self.list_of_points)):\n",
    "            # For all points, sum up the product of i-th basis function and i-th point.\n",
    "            x += basis_function[i] * self.list_of_points[i][0]\n",
    "            y += basis_function[i] * self.list_of_points[i][1]\n",
    "        return (x, y)\n",
    "\n",
    "    def plot_curve(self, step_size=0.01):\n",
    "        from matplotlib import pyplot as plt  # type: ignore\n",
    "\n",
    "        to_plot_x = []  # x coordinates of points to plot\n",
    "        to_plot_y = []  # y coordinates of points to plot\n",
    "\n",
    "        t = 0.0\n",
    "        while t <= 1:\n",
    "            value = self.bezier_curve_function(t)\n",
    "            to_plot_x.append(value[0])\n",
    "            to_plot_y.append(value[1])\n",
    "            t += step_size\n",
    "\n",
    "        x = [i[0] for i in self.list_of_points]\n",
    "        y = [i[1] for i in self.list_of_points]\n",
    "\n",
    "        plt.plot(\n",
    "            to_plot_x,\n",
    "            to_plot_y,\n",
    "            color=\"blue\",\n",
    "            label=\"Curve of Degree \" + str(self.degree),\n",
    "        )\n",
    "        plt.scatter(x, y, color=\"red\", label=\"Control Points\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def outcurve(self):\n",
    "        from matplotlib import pyplot as plt  # type: ignore\n",
    "\n",
    "        to_plot_x = []  # x coordinates of points to plot\n",
    "        to_plot_y = []  # y coordinates of points to plot\n",
    "\n",
    "        t = 0.0\n",
    "        while t <= 1:\n",
    "            value = self.bezier_curve_function(t)\n",
    "            to_plot_x.append(value[0])\n",
    "            to_plot_y.append(value[1])\n",
    "            t += 0.001\n",
    "\n",
    "        x = [i[0] for i in self.list_of_points]\n",
    "        y = [i[1] for i in self.list_of_points]\n",
    "        print(len(to_plot_y))\n",
    "        print(to_plot_x)\n",
    "        plt.plot(\n",
    "            to_plot_x,\n",
    "            to_plot_y,\n",
    "            color=\"blue\",\n",
    "            label=\"Curve of Degree \" + str(self.degree),\n",
    "        )\n",
    "        plt.scatter(x, y, color=\"red\", label=\"Control Points\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BezierCurve([(0, 2), (2, 2), (0, 0), (2, 0)]).plot_curve()  # degree 3\n",
    "BezierCurve([(0, 2), (200, 2), (0, 0), (1000, 0)]).outcurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "CreditsChTop",
    "TutorialTop",
    "CheckGPU",
    "InstallDeps",
    "DefMidasFns",
    "DefFns",
    "DefSecModel",
    "DefSuperRes",
    "AnimSetTop",
    "ExtraSetTop",
    "InstallRAFT",
    "CustModel",
    "FlowFns1",
    "FlowFns2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Disco Diffusion v5.6 [Now with portrait_generator_v001]",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
