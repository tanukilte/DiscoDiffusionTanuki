{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !zip -r \"/content/drive/MyDrive/AI/Disco_Diffusion/DDT-$(date +\"%Y-%m-%d\").zip\" /content/drive/MyDrive/AI/Disco_Diffusion/images_out\n",
    "except:\n",
    "    print(\"Not in Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !rm -rf '/content/drive/MyDrive/AI/Disco_Diffusion/images_out'\n",
    "except:\n",
    "    print(\"Not in Colab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SetupTop"
   },
   "source": [
    "# 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "CheckGPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 21 08:00:35 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:0A:00.0  On |                  N/A |\r\n",
      "|  0%   48C    P8    19W / 380W |    754MiB / 10240MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1476    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      2180    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "|    0   N/A  N/A      5344    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A      7384    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      7600    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n",
      "|    0   N/A  N/A     10420    C+G   ...artMenuExperienceHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A     10616    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A     10964    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n",
      "|    0   N/A  N/A     12092    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "|    0   N/A  N/A     12948    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\n",
      "ECC features not supported for GPU 00000000:0A:00.0.\r\n",
      "Treating as warning and moving on.\r\n",
      "All done.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title 1.1 Check GPU Status\n",
    "import subprocess\n",
    "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
    "if simple_nvidia_smi_display:\n",
    "    #!nvidia-smi\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "else:\n",
    "    #!nvidia-smi -i 0 -e 0\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_ecc_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "PrepFolders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Colab not detected.\n"
     ]
    }
   ],
   "source": [
    "#@title 1.2 Prepare Folders\n",
    "import subprocess, os, sys, ipykernel, requests\n",
    "\n",
    "def gitclone(url, targetdir=None):\n",
    "    if targetdir:\n",
    "        res = subprocess.run(['git', 'clone', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def fetchsave(url, path):\n",
    "    if is_colab:\n",
    "        res = subprocess.run(['wget', url, '-O', f'{path}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "        print(res)\n",
    "    else:\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(fetch(url).getbuffer())\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith(\n",
    "            'https://'):\n",
    "        print(f'Fetching {str(url_or_path)}. \\nThis might take a while... please wait.')\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def pipi(modulestr):\n",
    "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def pipie(modulestr):\n",
    "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"Google Colab detected. Using Google Drive.\")\n",
    "    is_colab = True\n",
    "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
    "    google_drive = True #@param {type:\"boolean\"}\n",
    "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
    "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
    "except:\n",
    "    is_colab = False\n",
    "    google_drive = False\n",
    "    save_models_to_google_drive = False\n",
    "    print(\"Google Colab not detected.\")\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive is True:\n",
    "        drive.mount('/content/drive')\n",
    "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
    "    else:\n",
    "        root_path = '/content'\n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "\n",
    "import os\n",
    "def createPath(filepath):\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "initDirPath = f'{root_path}/init_images'\n",
    "createPath(initDirPath)\n",
    "outDirPath = f'{root_path}/images_out'\n",
    "createPath(outDirPath)\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
    "        model_path = '/content/models'\n",
    "        createPath(model_path)\n",
    "    if google_drive and save_models_to_google_drive:\n",
    "        model_path = f'{root_path}/models'\n",
    "        createPath(model_path)\n",
    "else:\n",
    "    model_path = f'{root_path}/models'\n",
    "    createPath(model_path)\n",
    "\n",
    "# libraries = f'{root_path}/libraries'\n",
    "# createPath(libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "InstallDeps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lpips in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.1.4)\r\n",
      "Requirement already satisfied: datetime in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (4.5)\r\n",
      "Requirement already satisfied: timm in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.6.5)\r\n",
      "Requirement already satisfied: ftfy in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (6.1.1)\r\n",
      "Requirement already satisfied: einops in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.4.1)\r\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (1.6.5)\r\n",
      "Requirement already satisfied: omegaconf in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (2.2.2)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.10.2+cu113)\r\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (0.11.3+cu113)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.7.3)\r\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (4.64.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.21.6)\r\n",
      "Requirement already satisfied: pytz in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (2022.1)\r\n",
      "Requirement already satisfied: zope.interface in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (5.4.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from ftfy) (0.2.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (4.3.0)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (21.3)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.9.2)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2022.5.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (3.19.4)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.3.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from omegaconf) (4.9.3)\r\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\r\n",
      "Requirement already satisfied: requests in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.28.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (61.2.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.47.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.2)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.1.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from torchvision>=0.2.1->lpips) (9.2.0)\r\n",
      "Requirement already satisfied: colorama in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.5)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\r\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.10)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#@title ### 1.3 Install, import dependencies and set up runtime devices\n",
    "\n",
    "import pathlib, shutil, os, sys\n",
    "\n",
    "# There are some reports that with a T4 or V100 on Colab, downgrading to a previous version of PyTorch may be necessary.\n",
    "# .. but there are also reports that downgrading breaks them!  If you're facing issues, you may want to try uncommenting and running this code.\n",
    "# nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "# cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
    "# if is_colab:\n",
    "#     if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
    "#         print(\"Downgrading pytorch. This can take a couple minutes ...\")\n",
    "#         downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "#         print(\"pytorch downgraded.\")\n",
    "\n",
    "#@markdown Check this if you want to use CPU\n",
    "useCPU = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not is_colab:\n",
    "    # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "USE_ADABINS = True\n",
    "\n",
    "if is_colab:\n",
    "    if not google_drive:\n",
    "        root_path = f'/content'\n",
    "        model_path = '/content/models' \n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "    model_path = f'{root_path}/models'\n",
    "\n",
    "multipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "print(multipip_res)\n",
    "\n",
    "if is_colab:\n",
    "    subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "try:\n",
    "    from CLIP import clip\n",
    "except:\n",
    "    if not os.path.exists(\"CLIP\"):\n",
    "        gitclone(\"https://github.com/openai/CLIP\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
    "\n",
    "try:\n",
    "    import open_clip\n",
    "except:\n",
    "    if not os.path.exists(\"open_clip/src\"):\n",
    "        gitclone(\"https://github.com/mlfoundations/open_clip.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/open_clip/src')\n",
    "    import open_clip\n",
    "\n",
    "try:\n",
    "    from guided_diffusion.script_util import create_model_and_diffusion\n",
    "except:\n",
    "    if not os.path.exists(\"guided-diffusion\"):\n",
    "        gitclone(\"https://github.com/tanukilte/guided-diffusion\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n",
    "\n",
    "try:\n",
    "    from resize_right import resize\n",
    "except:\n",
    "    if not os.path.exists(\"ResizeRight\"):\n",
    "        gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n",
    "\n",
    "\n",
    "if not os.path.exists(\"DiscoDiffusion\"):\n",
    "    gitclone(\"https://github.com/tanukilte/DiscoDiffusionTanuki.git\")\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import timm\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from glob import glob\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "from CLIP import clip\n",
    "from resize_right import resize\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ipywidgets import Output\n",
    "import hashlib\n",
    "from functools import partial\n",
    "if is_colab:\n",
    "    os.chdir('/content')\n",
    "    from google.colab import files\n",
    "else:\n",
    "    os.chdir(f'{PROJECT_DIR}')\n",
    "from IPython.display import Image as ipyimg\n",
    "from numpy import asarray\n",
    "from einops import rearrange, repeat\n",
    "import torch, torchvision\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device('cuda:0' if (torch.cuda.is_available() and not useCPU) else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "device = DEVICE # At least one of the modules expects this name..\n",
    "\n",
    "if not useCPU:\n",
    "    if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
    "        print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        \n",
    "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "cutout_debug = False\n",
    "padargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "DefFns"
   },
   "outputs": [],
   "source": [
    "#@title 1.5 Define necessary functions\n",
    "\n",
    "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def val_interpolate(x1, y1, x2, y2, x):\n",
    "    #Linear interpolation. Return y between y1 and y2 for the same position x is bettewen x1 and x2 \n",
    "    output = (y1* (x2 - x) + y2 * (x - x1))/(x2 - x1)\n",
    "    if type(y1) == int:\n",
    "        output = round(output) # return the proper type\n",
    "    return(output)\n",
    "\n",
    "def smooth_jazz(schedule):\n",
    "\n",
    "    # Take a list of numbers (i.e. an already-evaluated schedule),\n",
    "    # find the places where the number changes from one to the next, and smooth those transitions\n",
    "    newschedule = schedule.copy() #newschedule = [i for i in schedule]\n",
    "    markers = []\n",
    "    last_num = schedule[0]\n",
    "    # build a list of indicies of where the number changes\n",
    "    for i in range(1, len(schedule)):\n",
    "        current_num = schedule[i]\n",
    "        if current_num != last_num:\n",
    "            markers.append(i)\n",
    "        last_num = current_num\n",
    "    # now smooth out the surrounding numbers for any markers we have\n",
    "    lastindex = 0\n",
    "    for i in range(len(markers)):\n",
    "        for k in range(lastindex, markers[i]):\n",
    "            newschedule[k] = val_interpolate(lastindex, schedule[lastindex], markers[i], schedule[markers[i]], k)\n",
    "        lastindex = markers[i]\n",
    "    return(newschedule)\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from matplotlib import pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "def bernstein_poly(i, n, t):\n",
    "    return comb(n, i) * ( t**i ) * (1 - t)**(n-i)\n",
    "\n",
    "\n",
    "def bezier_curve(points, isInt=False):\n",
    "    nPoints = len(points)\n",
    "    xPoints = np.array([p[0] for p in points])\n",
    "    yPoints = np.array([p[1] for p in points])\n",
    "    curveEnd = 1000 - points[-1][0]\n",
    "    t = np.linspace(0.0, 1.0, 1000 - curveEnd)\n",
    "\n",
    "    polynomial_array = np.array([ bernstein_poly(i, nPoints-1, t) for i in range(0, nPoints)   ])\n",
    "\n",
    "    #xvals = np.dot(xPoints, polynomial_array)\n",
    "    xvals = [i for i in range(1000)]\n",
    "    yvals = np.dot(yPoints, polynomial_array)\n",
    "    if isInt:\n",
    "        yvals = [round(i) for i in yvals]\n",
    "    if curveEnd > 0:\n",
    "        yvals = list(yvals) + [yvals[-1] for i in range(1000 - curveEnd, 1000)]\n",
    "    return xvals, yvals\n",
    "\n",
    "def plot_bezier(points, isInt=False, reg=None):\n",
    "    xpoints = [p[0] for p in points]\n",
    "    ypoints = [p[1] for p in points]\n",
    "    name = [name for name in globals() if globals()[name] is points]\n",
    "    if name is not None:\n",
    "        if len(name) > 0:\n",
    "            plt.title(name[0])\n",
    "    xvals, yvals = bezier_curve(points, isInt=isInt)\n",
    "    plt.plot(xvals, yvals)\n",
    "    plt.plot(xpoints, ypoints, \"ro\")\n",
    "    for nr in range(len(points)):\n",
    "        plt.text(points[nr][0], points[nr][1], nr)\n",
    "    if reg is not None:\n",
    "        x1 = [i for i in range(1000)]\n",
    "        plt.plot(x1, reg, label = \"line 1\")   \n",
    "    \n",
    "def perlin(width, height, scale=10, device=None):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 3):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale, device)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
    "    else:\n",
    "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
    "        out = TF.resize(size=(side_y, side_x), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "\n",
    "    out = ImageOps.autocontrast(out)\n",
    "    return out\n",
    "\n",
    "def regen_perlin():\n",
    "    if perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(batch_size, -1, -1, -1)\n",
    "\n",
    "def read_image_workaround(path):\n",
    "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
    "    this incompatibility to avoid colour inversions.\"\"\"\n",
    "    im_tmp = cv2.imread(path)\n",
    "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.reshape([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.reshape([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.skip_augs = skip_augs\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.15),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "\n",
    "        cutouts = []\n",
    "        for ch in range(self.cutn):\n",
    "            if ch > self.cutn - self.cutn//4:\n",
    "                cutout = input.clone()\n",
    "            else:\n",
    "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
    "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
    "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "\n",
    "            if not self.skip_augs:\n",
    "                cutout = self.augs(cutout)\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "            del cutout\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "class MakeCutoutsDango(nn.Module):\n",
    "    def __init__(self, cut_size,\n",
    "                 Overview=4, \n",
    "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.Overview = Overview\n",
    "        self.InnerCrop = InnerCrop\n",
    "        self.IC_Size_Pow = IC_Size_Pow\n",
    "        self.IC_Grey_P = IC_Grey_P\n",
    "        if args.animation_mode == 'None':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif args.animation_mode == 'Video Input':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.15),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.4),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "            ])\n",
    "          \n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        gray = T.Grayscale(3)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        l_size = max(sideX, sideY)\n",
    "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
    "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
    "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
    "        #MOD more useful cuts\n",
    "        if self.Overview > 0:\n",
    "            for i in range(self.Overview):\n",
    "                cutout = resize(pad_input, out_shape=output_shape)\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(gray(TF.hflip(cutout)))\n",
    "                    else:\n",
    "                        cutouts.append(gray(cutout))\n",
    "                else:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(TF.hflip(cutout))\n",
    "                    else:\n",
    "                        cutouts.append(cutout)\n",
    "\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
    "\n",
    "                              \n",
    "        if self.InnerCrop >0:\n",
    "            for i in range(self.InnerCrop):\n",
    "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
    "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "                offsety = torch.randint(0, sideY - size + 1, ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    cutout = gray(cutout)\n",
    "                cutout = resize(cutout, out_shape=output_shape)\n",
    "                cutouts.append(cutout)\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"/content/cutout_InnerCrop.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
    "        cutouts = torch.cat(cutouts)\n",
    "        if skip_augs is not True:\n",
    "            for i in range(cutouts.shape[0]):\n",
    "                cutouts[i]=self.augs(cutouts[i])\n",
    "        return cutouts\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def symmetry_transformation_fn(x):\n",
    "    if args.use_horizontal_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :, :w//2], torch.flip(x[:, :, :, :w//2], [-1])), -1)\n",
    "        print(\"horizontal symmetry applied\")\n",
    "    if args.use_vertical_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :h//2, :], torch.flip(x[:, :, :h//2, :], [-2])), -2)\n",
    "        print(\"vertical symmetry applied\")\n",
    "    return x\n",
    "\n",
    "def do_run():\n",
    "    seed = args.seed# + batch_num\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if args.init_image in ['','none', 'None', 'NONE']:\n",
    "        init_image = None\n",
    "    else:\n",
    "        init_image = args.init_image\n",
    "        init_scale = args.init_scale\n",
    "        skip_steps = args.skip_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    if args.prompts_series is not None:\n",
    "        prompts = args.prompts_series\n",
    "    else:\n",
    "        prompts = []\n",
    "\n",
    "    \n",
    "    if args.image_prompts_series is not None:\n",
    "        print(args.image_prompts_series)\n",
    "        image_prompts = args.image_prompts_series\n",
    "    else:\n",
    "        image_prompts = []\n",
    "\n",
    "    model_stats = []\n",
    "    for clip_model in clip_models:\n",
    "        cutn = 16\n",
    "        model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n",
    "        model_stat[\"clip_model\"] = clip_model\n",
    "\n",
    "        for prompt in prompts:\n",
    "            txt, weight = parse_prompt(prompt)\n",
    "            txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "\n",
    "            if args.fuzzy_prompt:\n",
    "                for i in range(25):\n",
    "                    model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n",
    "                    model_stat[\"weights\"].append(weight)\n",
    "            else:\n",
    "                model_stat[\"target_embeds\"].append(txt)\n",
    "                model_stat[\"weights\"].append(weight)\n",
    "\n",
    "        if image_prompts:\n",
    "            model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n",
    "            for prompt in image_prompt:\n",
    "                path, weight = parse_prompt(prompt)\n",
    "                img = Image.open(fetch(path)).convert('RGB')\n",
    "                img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
    "                batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "                embed = clip_model.encode_image(normalize(batch)).float()\n",
    "                if fuzzy_prompt:\n",
    "                    for i in range(25):\n",
    "                        model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n",
    "                        weights.extend([weight / cutn] * cutn)\n",
    "                else:\n",
    "                    model_stat[\"target_embeds\"].append(embed)\n",
    "                    model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
    "\n",
    "        model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
    "        model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
    "        if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
    "            raise RuntimeError('The weights must not sum to 0.')\n",
    "        model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
    "        model_stats.append(model_stat)\n",
    "\n",
    "        init = None\n",
    "        if init_image is not None:\n",
    "            init = Image.open(fetch(init_image)).convert('RGB')\n",
    "            init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
    "            init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        #init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, y=None):\n",
    "        with torch.enable_grad():\n",
    "            x_is_NaN = False\n",
    "            x = x.detach().requires_grad_()\n",
    "            n = x.shape[0]\n",
    "            if use_secondary_model is True:\n",
    "                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
    "                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "            else:\n",
    "                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "                \n",
    "            #MOD this whole chunk and spliting off cuts outer/inner:\n",
    "            normalizeGuideToRes = 1#width_height[0] * width_height[1] / 425984.0 #Normalize to 832,512 res\n",
    "                \n",
    "            for model_stat in model_stats:\n",
    "                t_int = int(t.item())+1 #errors on last step without +1, need to find source\n",
    "                #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
    "                try:\n",
    "                    input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n",
    "                except:\n",
    "                    input_resolution=224\n",
    "                for i in range(args.cutn_batches[1000-t_int]):\n",
    "                    cuts = MakeCutoutsDango(input_resolution,\n",
    "                            Overview= args.cut_overview[1000-t_int], \n",
    "                            InnerCrop =args.cut_innercut[1000-t_int],\n",
    "                            IC_Size_Pow=args.cut_ic_pow[1000-t_int],\n",
    "                            IC_Grey_P = args.cut_icgray_p[1000-t_int]\n",
    "                            )\n",
    "                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
    "                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
    "                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
    "                    dists = dists.view([args.cut_overview[1000-t_int] + args.cut_innercut[1000-t_int], n, -1])\n",
    "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
    "                    #loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n",
    "                    x_in_grad += torch.autograd.grad(losses.sum() * args.clip_guidance_scale[1000-t_int], x_in)[0] #/ args.cutn_batches[1000-t_int]\n",
    "            tv_losses = tv_loss(x_in)\n",
    "            if use_secondary_model is True:\n",
    "                range_losses = range_loss(out)\n",
    "            else:\n",
    "                range_losses = range_loss(out['pred_xstart'])\n",
    "            sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
    "            loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "            #if init is not None and init_scale:\n",
    "                #init_losses = lpips_model(x_in, init)\n",
    "                #loss = loss + init_losses.sum() * init_scale\n",
    "            x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "            if torch.isnan(x_in_grad).any()==False:\n",
    "                grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "            else:\n",
    "                # print(\"NaN'd\")\n",
    "                x_is_NaN = True\n",
    "                grad = torch.zeros_like(x)\n",
    "        if args.clamp_grad and x_is_NaN == False:\n",
    "            magnitude = grad.square().mean().sqrt()\n",
    "            grad = grad * magnitude.clamp(max=args.clamp_max[1000 - t_int]) / magnitude  #min=-0.02, min=-clamp_max, \n",
    "        #MOD:    \n",
    "        #Static thresholding: We refer to elementwise clipping the x-prediction to [−1, 1] as static thresholding. This method was in fact used but not emphasized in previous work [28 ], and to our knowledge its importance has not been investigated in the context of guided sampling. We discover that static thresholding is essential to sampling with large guidance weights and prevents generation of blank images. Nonetheless, static thresholding still results in over-saturated and less detailed images as the guidance weight further increases.\n",
    "        #made dynamic clamp instead of regular clamp\n",
    "        return grad.div(max(grad.max(), 0.0 - grad.min(), 1.0))\n",
    "\n",
    "    if args.diffusion_sampling_mode == 'ddim':\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.plms_sample_loop_progressive\n",
    "\n",
    "\n",
    "    image_display = Output()\n",
    "    for i in range(args.n_batches):\n",
    "        display.clear_output(wait=True)\n",
    "        batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n",
    "        batchBar.n = i\n",
    "        batchBar.refresh()\n",
    "        print('')\n",
    "        display.display(image_display)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        cur_t = diffusion.num_timesteps - skip_steps - 1\n",
    "        total_steps = cur_t\n",
    "\n",
    "        if perlin_init:\n",
    "            init = regen_perlin()\n",
    "\n",
    "        if args.diffusion_sampling_mode == 'ddim':\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                eta=args.eta,\n",
    "                dynamicThreshold=args.dynamicThreshold,\n",
    "                transformation_fn=symmetry_transformation_fn,\n",
    "                transformation_percent=args.transformation_percent\n",
    "            )\n",
    "        else:\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                order=2,\n",
    "        )\n",
    "\n",
    "        # with run_display:\n",
    "        # display.clear_output(wait=True)\n",
    "        for j, sample in enumerate(samples):    \n",
    "            cur_t -= 1\n",
    "            intermediateStep = False\n",
    "            if args.steps_per_checkpoint is not None:\n",
    "                if j % steps_per_checkpoint == 0 and j > 0:\n",
    "                    intermediateStep = True\n",
    "            elif j in args.intermediate_saves:\n",
    "                intermediateStep = True\n",
    "            with image_display:\n",
    "                if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
    "                    for k, image in enumerate(sample['pred_xstart']):\n",
    "                        # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                        current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
    "                        percent = math.ceil(j/total_steps*100)\n",
    "                        if args.n_batches > 0:\n",
    "                            #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n",
    "                            if cur_t == -1 and args.intermediates_in_subfolder is True:\n",
    "                                save_num = f'{frame_num:04}' if args.animation_mode != \"None\" else i\n",
    "                                filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n",
    "                            else:\n",
    "                                #If we're working with percentages, append it\n",
    "                                if args.steps_per_checkpoint is not None:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n",
    "                                # Or else, iIf we're working with specific steps, append those\n",
    "                                else:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n",
    "                        image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
    "                        if j % args.display_rate == 0 or cur_t == -1:\n",
    "                            image.save('progress.png')\n",
    "                            display.clear_output(wait=True)\n",
    "                            display.display(display.Image('progress.png'))\n",
    "                        if args.steps_per_checkpoint is not None:\n",
    "                            if j % args.steps_per_checkpoint == 0 and j > 0:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        else:\n",
    "                            if j in args.intermediate_saves:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        if cur_t == -1:\n",
    "                            save_settings()\n",
    "                            image.save(f'{batchFolder}/{filename}')\n",
    "                    \n",
    "      #plt.plot(np.array(loss_values), 'r')\n",
    "\n",
    "def save_settings():\n",
    "    setting_list = {\n",
    "        'text_prompts': text_prompts,\n",
    "        'image_prompts': image_prompts,\n",
    "        'clip_guidance_scale': clip_guidance_scale,\n",
    "        'tv_scale': tv_scale,\n",
    "        'range_scale': range_scale,\n",
    "        'sat_scale': sat_scale,\n",
    "        'cutn_batches': cutn_batches,\n",
    "        'init_image': init_image,\n",
    "        'init_scale': init_scale,\n",
    "        'skip_steps': skip_steps,\n",
    "        'perlin_init': perlin_init,\n",
    "        'perlin_mode': perlin_mode,\n",
    "        'skip_augs': skip_augs,\n",
    "        'randomize_class': randomize_class,\n",
    "        'clip_denoised': clip_denoised,\n",
    "        'clamp_grad': clamp_grad,\n",
    "        'clamp_max': clamp_max,\n",
    "        'seed': seed,\n",
    "        'fuzzy_prompt': fuzzy_prompt,\n",
    "        'rand_mag': rand_mag,\n",
    "        'eta': eta,\n",
    "        \"dynamicThreshold\": dynamicThreshold,\n",
    "        'width': width_height[0],\n",
    "        'height': width_height[1],\n",
    "        'diffusion_model': diffusion_model,\n",
    "        'use_secondary_model': use_secondary_model,\n",
    "        'steps': steps,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "        'ViTB32': ViTB32,\n",
    "        'ViTB16': ViTB16,\n",
    "        'ViTL14': ViTL14,\n",
    "        'ViTL14_336px': ViTL14_336px,\n",
    "        'RN101': RN101,\n",
    "        'RN50': RN50,\n",
    "        'RN50x4': RN50x4,\n",
    "        'RN50x16': RN50x16,\n",
    "        'RN50x64': RN50x64,\n",
    "        'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "        'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "        'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "        'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "        'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "        'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "        'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "        'RN50_yffcc15m': RN50_yffcc15m,\n",
    "        'RN50_cc12m': RN50_cc12m,\n",
    "        'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "        'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "        'RN101_yfcc15m': RN101_yfcc15m,\n",
    "        'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "        'cut_overview': str(cut_overview),\n",
    "        'cut_innercut': str(cut_innercut),\n",
    "        'cut_ic_pow': str(cut_ic_pow),\n",
    "        'cut_icgray_p': str(cut_icgray_p),\n",
    "        'sampling_mode': sampling_mode,\n",
    "        'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "        'use_vertical_symmetry':use_vertical_symmetry,\n",
    "        'transformation_percent':transformation_percent,\n",
    "    }\n",
    "    # print('Settings:', setting_list)\n",
    "    with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "        json.dump(setting_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "DefSecModel"
   },
   "outputs": [],
   "source": [
    "#@title 1.6 Define the secondary diffusion model\n",
    "\n",
    "def append_dims(x, n):\n",
    "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
    "\n",
    "\n",
    "def expand_to_planes(x, shape):\n",
    "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
    "\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
    "\n",
    "\n",
    "def t_to_alpha_sigma(t):\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionOutput:\n",
    "    v: torch.Tensor\n",
    "    pred: torch.Tensor\n",
    "    eps: torch.Tensor\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, c),\n",
    "            ConvBlock(c, c),\n",
    "            SkipBlock([\n",
    "                nn.AvgPool2d(2),\n",
    "                ConvBlock(c, c * 2),\n",
    "                ConvBlock(c * 2, c * 2),\n",
    "                SkipBlock([\n",
    "                    nn.AvgPool2d(2),\n",
    "                    ConvBlock(c * 2, c * 4),\n",
    "                    ConvBlock(c * 4, c * 4),\n",
    "                    SkipBlock([\n",
    "                        nn.AvgPool2d(2),\n",
    "                        ConvBlock(c * 4, c * 8),\n",
    "                        ConvBlock(c * 8, c * 4),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                    ]),\n",
    "                    ConvBlock(c * 8, c * 4),\n",
    "                    ConvBlock(c * 4, c * 2),\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                ]),\n",
    "                ConvBlock(c * 4, c * 2),\n",
    "                ConvBlock(c * 2, c),\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            ]),\n",
    "            ConvBlock(c * 2, c),\n",
    "            nn.Conv2d(c, 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.down = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, cs[0]),\n",
    "            ConvBlock(cs[0], cs[0]),\n",
    "            SkipBlock([\n",
    "                self.down,\n",
    "                ConvBlock(cs[0], cs[1]),\n",
    "                ConvBlock(cs[1], cs[1]),\n",
    "                SkipBlock([\n",
    "                    self.down,\n",
    "                    ConvBlock(cs[1], cs[2]),\n",
    "                    ConvBlock(cs[2], cs[2]),\n",
    "                    SkipBlock([\n",
    "                        self.down,\n",
    "                        ConvBlock(cs[2], cs[3]),\n",
    "                        ConvBlock(cs[3], cs[3]),\n",
    "                        SkipBlock([\n",
    "                            self.down,\n",
    "                            ConvBlock(cs[3], cs[4]),\n",
    "                            ConvBlock(cs[4], cs[4]),\n",
    "                            SkipBlock([\n",
    "                                self.down,\n",
    "                                ConvBlock(cs[4], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[4]),\n",
    "                                self.up,\n",
    "                            ]),\n",
    "                            ConvBlock(cs[4] * 2, cs[4]),\n",
    "                            ConvBlock(cs[4], cs[3]),\n",
    "                            self.up,\n",
    "                        ]),\n",
    "                        ConvBlock(cs[3] * 2, cs[3]),\n",
    "                        ConvBlock(cs[3], cs[2]),\n",
    "                        self.up,\n",
    "                    ]),\n",
    "                    ConvBlock(cs[2] * 2, cs[2]),\n",
    "                    ConvBlock(cs[2], cs[1]),\n",
    "                    self.up,\n",
    "                ]),\n",
    "                ConvBlock(cs[1] * 2, cs[1]),\n",
    "                ConvBlock(cs[1], cs[0]),\n",
    "                self.up,\n",
    "            ]),\n",
    "            ConvBlock(cs[0] * 2, cs[0]),\n",
    "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffClipSetTop"
   },
   "source": [
    "# 2. Diffusion and CLIP model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ModelSettings"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512x512_diffusion_uncond_finetune_008100 already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "secondary already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\Zhenya\\.conda\\envs\\DiscoDiffusionTanuki\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#@markdown ####**Models Settings (note: For pixel art, the best is pixelartdiffusion_expanded):**\n",
    "diffusion_model = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\", \"portrait_generator_v001\", \"pixelartdiffusion_expanded\", \"pixel_art_diffusion_hard_256\", \"pixel_art_diffusion_soft_256\", \"pixelartdiffusion4k\", \"watercolordiffusion_2\", \"watercolordiffusion\", \"PulpSciFiDiffusion\", \"custom\"]\n",
    "\n",
    "use_secondary_model = True #@param {type: 'boolean'}\n",
    "diffusion_sampling_mode = 'ddim' #@param ['plms','ddim']\n",
    "#@markdown #####**Custom model:**\n",
    "custom_path = '/content/drive/MyDrive/deep_learning/ddpm/ema_0.9999_058000.pt'#@param {type: 'string'}\n",
    "\n",
    "#@markdown #####**CLIP settings:**\n",
    "use_checkpoint = True #@param {type: 'boolean'}\n",
    "ViTB32 = True #@param{type:\"boolean\"}\n",
    "ViTB16 = True #@param{type:\"boolean\"}\n",
    "ViTL14 = True #@param{type:\"boolean\"}\n",
    "ViTL14_336px = False #@param{type:\"boolean\"}\n",
    "RN101 = False #@param{type:\"boolean\"}\n",
    "RN50 = False #@param{type:\"boolean\"}\n",
    "RN50x4 = False #@param{type:\"boolean\"}\n",
    "RN50x16 = True #@param{type:\"boolean\"}\n",
    "RN50x64 = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown #####**OpenCLIP settings:**\n",
    "ViTB32_laion2b_e16 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_32 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "RN50_yffcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_cc12m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_cc12m = False #@param{type:\"boolean\"}\n",
    "RN101_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN101_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown If you're having issues with model downloads, check this to compare SHA's:\n",
    "check_model_SHA = False #@param{type:\"boolean\"}\n",
    "\n",
    "diff_model_map = {\n",
    "    '256x256_diffusion_uncond': { 'downloaded': False, 'sha': 'a37c32fffd316cd494cf3f35b339936debdc1576dad13fe57c42399a5dbc78b1', 'uri_list': ['https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt', 'https://www.dropbox.com/s/9tqnqo930mpnpcn/256x256_diffusion_uncond.pt'] },\n",
    "    '512x512_diffusion_uncond_finetune_008100': { 'downloaded': False, 'sha': '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648', 'uri_list': ['https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt', 'https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt'] },\n",
    "    'portrait_generator_v001': { 'downloaded': False, 'sha': 'b7e8c747af880d4480b6707006f1ace000b058dd0eac5bb13558ba3752d9b5b9', 'uri_list': ['https://huggingface.co/felipe3dartist/portrait_generator_v001/resolve/main/portrait_generator_v001_ema_0.9999_1MM.pt'] },\n",
    "    'pixelartdiffusion_expanded': { 'downloaded': False, 'sha': 'a73b40556634034bf43b5a716b531b46fb1ab890634d854f5bcbbef56838739a', 'uri_list': ['https://huggingface.co/KaliYuga/PADexpanded/resolve/main/PADexpanded.pt'] },\n",
    "    'pixel_art_diffusion_hard_256': { 'downloaded': False, 'sha': 'be4a9de943ec06eef32c65a1008c60ad017723a4d35dc13169c66bb322234161', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_hard_256/resolve/main/pixel_art_diffusion_hard_256.pt'] },\n",
    "    'pixel_art_diffusion_soft_256': { 'downloaded': False, 'sha': 'd321590e46b679bf6def1f1914b47c89e762c76f19ab3e3392c8ca07c791039c', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_soft_256/resolve/main/pixel_art_diffusion_soft_256.pt'] },\n",
    "    'pixelartdiffusion4k': { 'downloaded': False, 'sha': 'a1ba4f13f6dabb72b1064f15d8ae504d98d6192ad343572cc416deda7cccac30', 'uri_list': ['https://huggingface.co/KaliYuga/pixelartdiffusion4k/resolve/main/pixelartdiffusion4k.pt'] },\n",
    "    'watercolordiffusion_2': { 'downloaded': False, 'sha': '49c281b6092c61c49b0f1f8da93af9b94be7e0c20c71e662e2aa26fee0e4b1a9', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion_2/resolve/main/watercolordiffusion_2.pt'] },\n",
    "    'watercolordiffusion': { 'downloaded': False, 'sha': 'a3e6522f0c8f278f90788298d66383b11ac763dd5e0d62f8252c962c23950bd6', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion/resolve/main/watercolordiffusion.pt'] },\n",
    "    'PulpSciFiDiffusion': { 'downloaded': False, 'sha': 'b79e62613b9f50b8a3173e5f61f0320c7dbb16efad42a92ec94d014f6e17337f', 'uri_list': ['https://huggingface.co/KaliYuga/PulpSciFiDiffusion/resolve/main/PulpSciFiDiffusion.pt'] },\n",
    "    'secondary': { 'downloaded': False, 'sha': '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a', 'uri_list': ['https://the-eye.eu/public/AI/models/v-diffusion/secondary_model_imagenet_2.pth', 'https://ipfs.pollinations.ai/ipfs/bafybeibaawhhk7fhyhvmm7x24zwwkeuocuizbqbcg5nqx64jq42j75rdiy/secondary_model_imagenet_2.pth'] },\n",
    "}\n",
    "\n",
    "kaliyuga_pixel_art_model_names = ['pixelartdiffusion_expanded', 'pixel_art_diffusion_hard_256', 'pixel_art_diffusion_soft_256', 'pixelartdiffusion4k', 'PulpSciFiDiffusion']\n",
    "kaliyuga_watercolor_model_names = ['watercolordiffusion', 'watercolordiffusion_2']\n",
    "kaliyuga_pulpscifi_model_names = ['PulpSciFiDiffusion']\n",
    "diffusion_models_256x256_list = ['256x256_diffusion_uncond'] + kaliyuga_pixel_art_model_names + kaliyuga_watercolor_model_names + kaliyuga_pulpscifi_model_names\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_model_filename(diffusion_model_name):\n",
    "    model_uri = diff_model_map[diffusion_model_name]['uri_list'][0]\n",
    "    model_filename = os.path.basename(urlparse(model_uri).path)\n",
    "    return model_filename\n",
    "\n",
    "\n",
    "def download_model(diffusion_model_name, uri_index=0):\n",
    "    if diffusion_model_name != 'custom':\n",
    "        model_filename = get_model_filename(diffusion_model_name)\n",
    "        model_local_path = os.path.join(model_path, model_filename)\n",
    "        if os.path.exists(model_local_path) and check_model_SHA:\n",
    "            print(f'Checking {diffusion_model_name} File')\n",
    "            with open(model_local_path, \"rb\") as f:\n",
    "                bytes = f.read() \n",
    "                hash = hashlib.sha256(bytes).hexdigest()\n",
    "            if hash == diff_model_map[diffusion_model_name]['sha']:\n",
    "                print(f'{diffusion_model_name} SHA matches')\n",
    "                diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "            else:\n",
    "                print(f\"{diffusion_model_name} SHA doesn't match. Will redownload it.\")\n",
    "        elif os.path.exists(model_local_path) and not check_model_SHA or diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            print(f'{diffusion_model_name} already downloaded. If the file is corrupt, enable check_model_SHA.')\n",
    "            diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "\n",
    "        if not diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            for model_uri in diff_model_map[diffusion_model_name]['uri_list']:\n",
    "                fetchsave(model_uri, model_local_path)\n",
    "                if os.path.exists(model_local_path):\n",
    "                    diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "                    return\n",
    "                else:\n",
    "                    print(f'{diffusion_model_name} model download from {model_uri} failed. Will try any fallback uri.')\n",
    "            print(f'{diffusion_model_name} download failed.')\n",
    "\n",
    "\n",
    "# Download the diffusion model(s)\n",
    "download_model(diffusion_model)\n",
    "if use_secondary_model:\n",
    "    download_model('secondary')\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == '256x256_diffusion_uncond':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 256,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == 'portrait_generator_v001':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000,\n",
    "        'rescale_timesteps': True,\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 128,\n",
    "        'num_heads': 4,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "else:  # E.g. A model finetuned by KaliYuga\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "    \n",
    "if diffusion_model == 'custom':\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "\n",
    "model_default = model_config['image_size']\n",
    "\n",
    "if use_secondary_model:\n",
    "    secondary_model = SecondaryDiffusionImageNet2()\n",
    "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "    secondary_model.eval().requires_grad_(False).to(device)\n",
    "\n",
    "clip_models = []\n",
    "if ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion2b_e16: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion2b_e16').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_32: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if RN50_yffcc15m: clip_models.append(open_clip.create_model('RN50', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_cc12m: clip_models.append(open_clip.create_model('RN50', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_cc12m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN101_yfcc15m: clip_models.append(open_clip.create_model('RN101', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN101_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN101-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SettingsTop"
   },
   "source": [
    "# 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BasicSettings"
   },
   "outputs": [],
   "source": [
    "#@markdown ####**Basic Settings:**\n",
    "batch_name = 'DDT' #@param{type: 'string'}\n",
    "display_rate = 20 #@param{type: 'number'}\n",
    "n_batches = 5 #@param{type: 'number'}\n",
    "steps = 250 #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\n",
    "width_height_for_512x512_models = [1280, 768] #@param{type: 'raw'}\n",
    "clip_guidance_scale = [[0,16000],[500,16000],[1000,12000]] #@param{type: 'raw'}\n",
    "tv_scale = 0#@param{type: 'number'}\n",
    "range_scale = 150#@param{type: 'number'}\n",
    "sat_scale = 0#@param{type: 'number'}\n",
    "cutn_batches = [[0,1],[300,3],[1000,5]]#@param{type: 'raw'}\n",
    "skip_augs = False#@param{type: 'boolean'}\n",
    "dynamicThreshold = [[0,0.036],[500,0.088],[1000,0.088]]#@param{type: 'raw'}\n",
    "#@markdown ####**Image dimensions to be used for 256x256 models (e.g. pixelart models):**\n",
    "width_height_for_256x256_models = [512, 448] #@param{type: 'raw'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Init Image Settings:**\n",
    "init_image = None #@param{type: 'string'}\n",
    "init_scale = 1000 #@param{type: 'integer'}\n",
    "skip_steps = 0 #@param{type: 'integer'}\n",
    "#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n",
    "\n",
    "width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "\n",
    "#Get corrected sizes\n",
    "side_x = (width_height[0]//64)*64;\n",
    "side_y = (width_height[1]//64)*64;\n",
    "if side_x != width_height[0] or side_y != width_height[1]:\n",
    "    print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "\n",
    "#Make folder for batch\n",
    "batchFolder = f'{outDirPath}/{batch_name}'\n",
    "createPath(batchFolder)\n",
    "#@markdown ####**Saving:**\n",
    "\n",
    "intermediate_saves = [50, 100, 150, 200]#@param{type: 'raw'}\n",
    "intermediates_in_subfolder = True #@param{type: 'boolean'}\n",
    "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
    "\n",
    "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
    "\n",
    "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
    "\n",
    "\n",
    "if type(intermediate_saves) is not list:\n",
    "    if intermediate_saves:\n",
    "        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "        print(f'Will save every {steps_per_checkpoint} steps')\n",
    "    else:\n",
    "        steps_per_checkpoint = steps+10\n",
    "else:\n",
    "    steps_per_checkpoint = None\n",
    "\n",
    "if intermediate_saves and intermediates_in_subfolder is True:\n",
    "    partialFolder = f'{batchFolder}/partials'\n",
    "    createPath(partialFolder)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Advanced Settings:**\n",
    "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
    "\n",
    "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
    "\n",
    "perlin_init = True  #@param{type: 'boolean'}\n",
    "perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\n",
    "set_seed = 'random_seed' #@param{type: 'string'}\n",
    "eta = [[0,0.0],[0,0.64],[1000,0.64]]#@param{type: 'string'}\n",
    "clamp_grad = True #@param{type: 'boolean'}\n",
    "clamp_max = [[0,0.036],[500,0.12],[1000,0.064]] #@param{type: 'raw'}\n",
    "frames_skip_steps = \"0%\"#@param{type: 'string'}\n",
    "\n",
    "### EXTRA ADVANCED SETTINGS:\n",
    "randomize_class = True\n",
    "clip_denoised = False\n",
    "fuzzy_prompt = False\n",
    "rand_mag = 0.05\n",
    "sampling_mode = \"ddim\"#@param{type: 'string'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Cutn Scheduling:**\n",
    "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
    "\n",
    "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
    "\n",
    "cut_overview = [[0,12],[100,6],[1000,0]] #@param {type: 'raw'}       \n",
    "cut_innercut = [[0,0],[100,6],[1000,12]] #@param {type: 'raw'}  \n",
    "cut_ic_pow = [[0,0],[500,0.0],[1000,24]] #@param {type: 'raw'}  \n",
    "cut_icgray_p = [[0,0.25],[1000,0.25],[400,0]] #@param {type: 'raw'}\n",
    "\n",
    "#@markdown KaliYuga model settings. Refer to [cut_ic_pow](https://ezcharts.miraheze.org/wiki/Category:Cut_ic_pow) as a guide. Values between 1 and 100 all work.\n",
    "pad_or_pulp_cut_overview = \"[15]*100+[15]*100+[12]*100+[12]*100+[6]*100+[4]*100+[2]*200+[0]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_innercut = \"[1]*100+[1]*100+[4]*100+[4]*100+[8]*100+[8]*100+[10]*200+[10]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_icgray_p = \"[0.87]*100+[0.78]*50+[0.73]*50+[0.64]*60+[0.56]*40+[0.50]*50+[0.33]*100+[0.19]*150+[0]*400\" #@param {type: 'string'}\n",
    "\n",
    "watercolor_cut_overview = \"[14]*200+[12]*200+[4]*400+[0]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_innercut = \"[2]*200+[4]*200+[12]*400+[12]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "watercolor_cut_icgray_p = \"[0.7]*100+[0.6]*100+[0.45]*100+[0.3]*100+[0]*600\" #@param {type: 'string'}\n",
    "\n",
    "if (diffusion_model in kaliyuga_pixel_art_model_names) or (diffusion_model in kaliyuga_pulpscifi_model_names):\n",
    "    cut_overview = pad_or_pulp_cut_overview\n",
    "    cut_innercut = pad_or_pulp_cut_innercut\n",
    "    cut_ic_pow = pad_or_pulp_cut_ic_pow\n",
    "    cut_icgray_p = pad_or_pulp_cut_icgray_p\n",
    "elif diffusion_model in kaliyuga_watercolor_model_names:\n",
    "    cut_overview = watercolor_cut_overview\n",
    "    cut_innercut = watercolor_cut_innercut\n",
    "    cut_ic_pow = watercolor_cut_ic_pow\n",
    "    cut_icgray_p = watercolor_cut_icgray_p\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Transformation Settings:**\n",
    "use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
    "use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
    "transformation_percent = [0.09] #@param {type: 'raw'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Local Settings\n",
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    #Modifying settings for local notebook easier here\n",
    "    batch_name = 'DDT'\n",
    "    n_batches = 5\n",
    "    width_height_for_512x512_models = [960,576]\n",
    "    steps = 250\n",
    "    set_seed = 'random_seed'\n",
    "    display_rate = 20\n",
    "    intermediate_saves = [50, 100, 150, 200, 250, 300, 350, 400, 450]\n",
    "    intermediates_in_subfolder = True\n",
    "    clip_guidance_scale = [[0,16000],[500,16000],[1000,8000]]\n",
    "    frames_skip_steps = \"0%\"\n",
    "    tv_scale = 0\n",
    "    range_scale = 0\n",
    "    sat_scale = 0\n",
    "    cutn_batches = [[0,2],[300,4],[1000,5]]\n",
    "    cut_overview = [[0,6],[100,3],[1000,0]]      \n",
    "    cut_innercut = [[0,0],[100,3],[1000,6]]\n",
    "    cut_ic_pow = [[0,0],[500,0.0],[1000,24]]\n",
    "    cut_icgray_p = [[0,0.25],[1000,0.25],[400,0]]\n",
    "    clamp_grad = True\n",
    "    clamp_max = [[0,0.036],[500,0.12],[1000,0.064]]\n",
    "    eta = [[0,0.0],[0,0.64],[1000,0.5]]\n",
    "    perlin_init = True\n",
    "    perlin_mode = 'mixed'\n",
    "    skip_augs = False\n",
    "    use_vertical_symmetry = False\n",
    "    use_horizontal_symmetry = False\n",
    "    transformation_percent = [0.09]\n",
    "    dynamicThreshold=[[0,0.12],[0,1.1],[888,1.2],[1000,0.88]]\n",
    "\n",
    "    #Settings cleanup\n",
    "    width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "    side_x = (width_height[0]//64)*64;\n",
    "    side_y = (width_height[1]//64)*64;\n",
    "    if side_x != width_height[0] or side_y != width_height[1]:\n",
    "        print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "    batchFolder = f'{outDirPath}/{batch_name}'\n",
    "    createPath(batchFolder)\n",
    "    if type(intermediate_saves) is not list:\n",
    "        if intermediate_saves:\n",
    "            steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "            steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "            print(f'Will save every {steps_per_checkpoint} steps')\n",
    "        else:\n",
    "            steps_per_checkpoint = steps+10\n",
    "    else:\n",
    "        steps_per_checkpoint = None\n",
    "    if intermediate_saves and intermediates_in_subfolder is True:\n",
    "        partialFolder = f'{batchFolder}/partials'\n",
    "        createPath(partialFolder)\n",
    "finally:\n",
    "    pass\n",
    "    plt.rcParams[\"figure.figsize\"]=16,16      \n",
    "    plt.subplot(5, 2, 1)\n",
    "    plot_bezier(clip_guidance_scale, True)\n",
    "    plt.subplot(5, 2, 2)\n",
    "    plot_bezier(cutn_batches, True)\n",
    "    plt.subplot(5, 2, 3)\n",
    "    plot_bezier(cut_overview, True)\n",
    "    plt.subplot(5, 2, 4)\n",
    "    plot_bezier(cut_innercut, True)\n",
    "    plt.subplot(5, 2, 5)\n",
    "    plot_bezier(cut_ic_pow)\n",
    "    plt.subplot(5, 2, 6)\n",
    "    plot_bezier(clamp_max)\n",
    "    plt.subplot(5, 2, 7)\n",
    "    plot_bezier(cut_icgray_p)\n",
    "    plt.subplot(5, 2, 8)\n",
    "    plot_bezier(eta)\n",
    "    plt.subplot(5, 2, 9)\n",
    "    plot_bezier(dynamicThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PromptsTop"
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Prompts"
   },
   "outputs": [],
   "source": [
    "# Note: If using a pixelart diffusion model, try adding \"#pixelart\" to the end of the prompt for a stronger effect. It'll tend to work a lot better!\n",
    "text_prompts = {\n",
    "    0: [\"Ancient Pyramid of the Dragon Gods, by federico pelat and greg rutkowski, a beautifully ultradetailed painting of an ancient Egyptian pyramid that glows with a gentle noble aura with statues and monuments to dragon gods, trending on artstation, 4k, ultrawide lens:3\",\n",
    "            \"people, dof, blur:-1\",\n",
    "       ],\n",
    "}\n",
    "\n",
    "image_prompts = {\n",
    "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffuseTop"
   },
   "source": [
    "# 4. Diffuse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DoTheRun"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a09cac1deb64e9ea6430e22326cb7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82049c1d449a4840bdfd17d33ef11642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d691a7a45ef745b1ac5e0d3b26ec4abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed used: 3897095027\n"
     ]
    }
   ],
   "source": [
    "#@title Do the Run!\n",
    "#@markdown `n_batches` ignored with animation modes.\n",
    "\n",
    "#Update Model Settings\n",
    "timestep_respacing = f'ddim{steps}'\n",
    "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
    "model_config.update({\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "})\n",
    "\n",
    "batch_size = 1 \n",
    "\n",
    "def move_files(start_num, end_num, old_folder, new_folder):\n",
    "    for i in range(start_num, end_num):\n",
    "        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "resume_run = False #@param{type: 'boolean'}\n",
    "run_to_resume = 'latest' #@param{type: 'string'}\n",
    "resume_from_frame = 'latest' #@param{type: 'string'}\n",
    "retain_overwritten_frames = False #@param{type: 'boolean'}\n",
    "if retain_overwritten_frames:\n",
    "    retainFolder = f'{batchFolder}/retained'\n",
    "    createPath(retainFolder)\n",
    "\n",
    "\n",
    "skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
    "calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
    "\n",
    "if steps <= calc_frames_skip_steps:\n",
    "    sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
    "\n",
    "start_frame = 0\n",
    "batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
    "while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\"):\n",
    "    batchNum += 1\n",
    "\n",
    "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
    "\n",
    "if set_seed == 'random_seed':\n",
    "    random.seed()\n",
    "    seed = random.randint(0, 2**32)\n",
    "    # print(f'Using seed: {seed}')\n",
    "else:\n",
    "    seed = int(set_seed)\n",
    "\n",
    "    \n",
    "args = {\n",
    "    'batchNum': batchNum,\n",
    "    'prompts_series': text_prompts[0] if text_prompts else None,\n",
    "    'image_prompts_series':image_prompts[0] if image_prompts else None,\n",
    "    'seed': seed,\n",
    "    'display_rate':display_rate,\n",
    "    'n_batches':n_batches,\n",
    "    'batch_size':batch_size,\n",
    "    'batch_name': batch_name,\n",
    "    'steps': steps,\n",
    "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "    'width_height': width_height,\n",
    "    'clip_guidance_scale': bezier_curve(clip_guidance_scale, True)[1],\n",
    "    'tv_scale': tv_scale,\n",
    "    'range_scale': range_scale,\n",
    "    'sat_scale': sat_scale,\n",
    "    'cutn_batches': bezier_curve(cutn_batches, True)[1],\n",
    "    'init_image': init_image,\n",
    "    'init_scale': init_scale,\n",
    "    'skip_steps': skip_steps,\n",
    "    'side_x': side_x,\n",
    "    'side_y': side_y,\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'skip_step_ratio': skip_step_ratio,\n",
    "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
    "    'text_prompts': text_prompts,\n",
    "    'image_prompts': image_prompts,\n",
    "    'cut_overview': bezier_curve(cut_overview, True)[1],\n",
    "    'cut_innercut': bezier_curve(cut_innercut, True)[1],\n",
    "    'cut_ic_pow': bezier_curve(cut_ic_pow)[1],\n",
    "    'cut_icgray_p': bezier_curve(cut_icgray_p)[1],\n",
    "    'intermediate_saves': intermediate_saves,\n",
    "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
    "    'steps_per_checkpoint': steps_per_checkpoint,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'set_seed': set_seed,\n",
    "    'eta': bezier_curve(eta)[1],\n",
    "    \"dynamicThreshold\": bezier_curve(dynamicThreshold)[1],\n",
    "    'clamp_grad': clamp_grad,\n",
    "    'clamp_max': bezier_curve(clamp_max)[1],\n",
    "    'skip_augs': skip_augs,\n",
    "    'randomize_class': randomize_class,\n",
    "    'clip_denoised': clip_denoised,\n",
    "    'fuzzy_prompt': fuzzy_prompt,\n",
    "    'rand_mag': rand_mag,\n",
    "    'use_vertical_symmetry': use_vertical_symmetry,\n",
    "    'use_horizontal_symmetry': use_horizontal_symmetry,\n",
    "    'transformation_percent': transformation_percent,\n",
    "    'animation_mode': 'None',\n",
    "}\n",
    "\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "print('Prepping model...')\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "if diffusion_model == 'custom':\n",
    "    model.load_state_dict(torch.load(custom_path, map_location='cpu'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    do_run()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print('Seed used:', seed)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "CreditsChTop",
    "TutorialTop",
    "CheckGPU",
    "InstallDeps",
    "DefMidasFns",
    "DefFns",
    "DefSecModel",
    "DefSuperRes",
    "AnimSetTop",
    "ExtraSetTop",
    "InstallRAFT",
    "CustModel",
    "FlowFns1",
    "FlowFns2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Disco Diffusion v5.6 [Now with portrait_generator_v001]",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
