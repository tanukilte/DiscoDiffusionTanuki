{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !zip -r \"/content/drive/MyDrive/AI/Disco_Diffusion/DDT-$(date +\"%Y-%m-%d\").zip\" /content/drive/MyDrive/AI/Disco_Diffusion/images_out\n",
    "except:\n",
    "    print(\"Not in Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    !rm -rf '/content/drive/MyDrive/AI/Disco_Diffusion/images_out'\n",
    "except:\n",
    "    print(\"Not in Colab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SetupTop"
   },
   "source": [
    "# 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "CheckGPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 18 21:26:20 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:0A:00.0  On |                  N/A |\r\n",
      "|  0%   53C    P3    74W / 380W |   1296MiB / 10240MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1496    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      1612    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A      4612    C+G   Insufficient Permissions        N/A      |\r\n",
      "|    0   N/A  N/A      6564    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n",
      "|    0   N/A  N/A      8176    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n",
      "|    0   N/A  N/A     10852    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n",
      "|    0   N/A  N/A     10900    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\r\n",
      "|    0   N/A  N/A     13976    C+G   ...s\\Notepad++\\notepad++.exe    N/A      |\r\n",
      "|    0   N/A  N/A     14456    C+G   ...\\app-1.0.9005\\Discord.exe    N/A      |\r\n",
      "|    0   N/A  N/A     19552    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "|    0   N/A  N/A     20728    C+G   ...signal-desktop\\Signal.exe    N/A      |\r\n",
      "|    0   N/A  N/A     21540    C+G   ...zilla Firefox\\firefox.exe    N/A      |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "\n",
      "ECC features not supported for GPU 00000000:0A:00.0.\r\n",
      "Treating as warning and moving on.\r\n",
      "All done.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title 1.1 Check GPU Status\n",
    "import subprocess\n",
    "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
    "if simple_nvidia_smi_display:\n",
    "    #!nvidia-smi\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "else:\n",
    "    #!nvidia-smi -i 0 -e 0\n",
    "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_output)\n",
    "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(nvidiasmi_ecc_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "PrepFolders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Colab not detected.\n"
     ]
    }
   ],
   "source": [
    "#@title 1.2 Prepare Folders\n",
    "import subprocess, os, sys, ipykernel, requests\n",
    "\n",
    "def gitclone(url, targetdir=None):\n",
    "    if targetdir:\n",
    "        res = subprocess.run(['git', 'clone', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    else:\n",
    "        res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def fetchsave(url, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(fetch(url).getbuffer())\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith(\n",
    "            'https://'):\n",
    "        print(f'Fetching {str(url_or_path)}. \\nThis might take a while... please wait.')\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def pipi(modulestr):\n",
    "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "def pipie(modulestr):\n",
    "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(res)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"Google Colab detected. Using Google Drive.\")\n",
    "    is_colab = True\n",
    "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
    "    google_drive = True #@param {type:\"boolean\"}\n",
    "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
    "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
    "except:\n",
    "    is_colab = False\n",
    "    google_drive = False\n",
    "    save_models_to_google_drive = False\n",
    "    print(\"Google Colab not detected.\")\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive is True:\n",
    "        drive.mount('/content/drive')\n",
    "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
    "    else:\n",
    "        root_path = '/content'\n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "\n",
    "import os\n",
    "def createPath(filepath):\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "initDirPath = f'{root_path}/init_images'\n",
    "createPath(initDirPath)\n",
    "outDirPath = f'{root_path}/images_out'\n",
    "createPath(outDirPath)\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "if is_colab:\n",
    "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
    "        model_path = '/content/models'\n",
    "        createPath(model_path)\n",
    "    if google_drive and save_models_to_google_drive:\n",
    "        model_path = f'{root_path}/models'\n",
    "        createPath(model_path)\n",
    "else:\n",
    "    model_path = f'{root_path}/models'\n",
    "    createPath(model_path)\n",
    "\n",
    "# libraries = f'{root_path}/libraries'\n",
    "# createPath(libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disco_xform_utils.py failed to import InferenceHelper. Please ensure that AdaBins directory is in the path (i.e. via sys.path.append('./AdaBins') or other means).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from CLIP import clip\n",
    "except:\n",
    "    if not os.path.exists(\"CLIP\"):\n",
    "        gitclone(\"https://github.com/openai/CLIP\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
    "\n",
    "try:\n",
    "    import open_clip\n",
    "except:\n",
    "    if not os.path.exists(\"open_clip/src\"):\n",
    "        gitclone(\"https://github.com/mlfoundations/open_clip.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/open_clip/src')\n",
    "    import open_clip\n",
    "\n",
    "try:\n",
    "    from guided_diffusion.script_util import create_model_and_diffusion\n",
    "except:\n",
    "    if not os.path.exists(\"guided-diffusion\"):\n",
    "        gitclone(\"https://github.com/tanukilte/guided-diffusion\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n",
    "\n",
    "try:\n",
    "    from resize_right import resize\n",
    "except:\n",
    "    if not os.path.exists(\"ResizeRight\"):\n",
    "        gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n",
    "    sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n",
    "\n",
    "try:\n",
    "    sys.path.append(PROJECT_DIR)\n",
    "    import disco_xform_utils as dxf\n",
    "except:\n",
    "    if not os.path.exists(\"DiscoDiffusion\"):\n",
    "        gitclone(\"https://github.com/tanukilte/DiscoDiffusionTanuki.git\")\n",
    "    if not os.path.exists('disco_xform_utils.py'):\n",
    "        shutil.move('disco-diffusion/disco_xform_utils.py', 'disco_xform_utils.py')\n",
    "    sys.path.append(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "InstallDeps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lpips in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.1.4)\r\n",
      "Requirement already satisfied: datetime in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (4.5)\r\n",
      "Requirement already satisfied: timm in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.6.5)\r\n",
      "Requirement already satisfied: ftfy in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (6.1.1)\r\n",
      "Requirement already satisfied: einops in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (0.4.1)\r\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (1.6.5)\r\n",
      "Requirement already satisfied: omegaconf in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (2.2.2)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.10.2+cu113)\r\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (4.64.0)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.7.3)\r\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (0.11.3+cu113)\r\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from lpips) (1.21.6)\r\n",
      "Requirement already satisfied: pytz in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (2022.1)\r\n",
      "Requirement already satisfied: zope.interface in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from datetime) (5.4.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from ftfy) (0.2.5)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.9.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (4.3.0)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (2022.5.0)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (0.3.2)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (3.19.4)\r\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pytorch-lightning) (21.3)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from omegaconf) (4.9.3)\r\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\r\n",
      "Requirement already satisfied: requests in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.28.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.47.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.1.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.9.1)\r\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (61.2.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from torchvision>=0.2.1->lpips) (9.2.0)\r\n",
      "Requirement already satisfied: colorama in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.10)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zhenya\\.conda\\envs\\discodiffusiontanuki\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#@title ### 1.3 Install, import dependencies and set up runtime devices\n",
    "\n",
    "import pathlib, shutil, os, sys\n",
    "\n",
    "# There are some reports that with a T4 or V100 on Colab, downgrading to a previous version of PyTorch may be necessary.\n",
    "# .. but there are also reports that downgrading breaks them!  If you're facing issues, you may want to try uncommenting and running this code.\n",
    "# nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "# cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
    "# if is_colab:\n",
    "#     if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
    "#         print(\"Downgrading pytorch. This can take a couple minutes ...\")\n",
    "#         downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "#         print(\"pytorch downgraded.\")\n",
    "\n",
    "#@markdown Check this if you want to use CPU\n",
    "useCPU = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not is_colab:\n",
    "    # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "USE_ADABINS = True\n",
    "\n",
    "if is_colab:\n",
    "    if not google_drive:\n",
    "        root_path = f'/content'\n",
    "        model_path = '/content/models' \n",
    "else:\n",
    "    root_path = os.getcwd()\n",
    "    model_path = f'{root_path}/models'\n",
    "\n",
    "multipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "print(multipip_res)\n",
    "\n",
    "if is_colab:\n",
    "    subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import timm\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from glob import glob\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "from CLIP import clip\n",
    "from resize_right import resize\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ipywidgets import Output\n",
    "import hashlib\n",
    "from functools import partial\n",
    "if is_colab:\n",
    "    os.chdir('/content')\n",
    "    from google.colab import files\n",
    "else:\n",
    "    os.chdir(f'{PROJECT_DIR}')\n",
    "from IPython.display import Image as ipyimg\n",
    "from numpy import asarray\n",
    "from einops import rearrange, repeat\n",
    "import torch, torchvision\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# AdaBins stuff\n",
    "if USE_ADABINS:\n",
    "    \n",
    "    from infer import InferenceHelper\n",
    "    MAX_ADABINS_AREA = 500000\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device('cuda:0' if (torch.cuda.is_available() and not useCPU) else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "device = DEVICE # At least one of the modules expects this name..\n",
    "\n",
    "if not useCPU:\n",
    "    if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
    "        print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        \n",
    "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
    "TRANSLATION_SCALE = 1.0/200.0\n",
    "cutout_debug = False\n",
    "padargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "DefFns"
   },
   "outputs": [],
   "source": [
    "#@title 1.5 Define necessary functions\n",
    "\n",
    "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
    "\n",
    "import disco_xform_utils as dxf\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def val_interpolate(x1, y1, x2, y2, x):\n",
    "    #Linear interpolation. Return y between y1 and y2 for the same position x is bettewen x1 and x2 \n",
    "    output = (y1* (x2 - x) + y2 * (x - x1))/(x2 - x1)\n",
    "    if type(y1) == int:\n",
    "        output = round(output) # return the proper type\n",
    "    return(output)\n",
    "\n",
    "def smooth_jazz(schedule):\n",
    "\n",
    "    # Take a list of numbers (i.e. an already-evaluated schedule),\n",
    "    # find the places where the number changes from one to the next, and smooth those transitions\n",
    "    newschedule = schedule.copy() #newschedule = [i for i in schedule]\n",
    "    markers = []\n",
    "    last_num = schedule[0]\n",
    "    # build a list of indicies of where the number changes\n",
    "    for i in range(1, len(schedule)):\n",
    "        current_num = schedule[i]\n",
    "        if current_num != last_num:\n",
    "            markers.append(i)\n",
    "        last_num = current_num\n",
    "    # now smooth out the surrounding numbers for any markers we have\n",
    "    lastindex = 0\n",
    "    for i in range(len(markers)):\n",
    "        for k in range(lastindex, markers[i]):\n",
    "            newschedule[k] = val_interpolate(lastindex, schedule[lastindex], markers[i], schedule[markers[i]], k)\n",
    "        lastindex = markers[i]\n",
    "    return(newschedule)\n",
    "\n",
    "def perlin(width, height, scale=10, device=None):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 3):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale, device)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
    "    else:\n",
    "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
    "        out = TF.resize(size=(side_y, side_x), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "\n",
    "    out = ImageOps.autocontrast(out)\n",
    "    return out\n",
    "\n",
    "def regen_perlin():\n",
    "    if perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(batch_size, -1, -1, -1)\n",
    "\n",
    "def read_image_workaround(path):\n",
    "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
    "    this incompatibility to avoid colour inversions.\"\"\"\n",
    "    im_tmp = cv2.imread(path)\n",
    "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.reshape([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.reshape([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.skip_augs = skip_augs\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.15),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "\n",
    "        cutouts = []\n",
    "        for ch in range(self.cutn):\n",
    "            if ch > self.cutn - self.cutn//4:\n",
    "                cutout = input.clone()\n",
    "            else:\n",
    "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
    "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
    "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "\n",
    "            if not self.skip_augs:\n",
    "                cutout = self.augs(cutout)\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "            del cutout\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "class MakeCutoutsDango(nn.Module):\n",
    "    def __init__(self, cut_size,\n",
    "                 Overview=4, \n",
    "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.Overview = Overview\n",
    "        self.InnerCrop = InnerCrop\n",
    "        self.IC_Size_Pow = IC_Size_Pow\n",
    "        self.IC_Grey_P = IC_Grey_P\n",
    "        if args.animation_mode == 'None':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif args.animation_mode == 'Video Input':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.15),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            ])\n",
    "        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n",
    "            self.augs = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.4),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "            ])\n",
    "          \n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        gray = T.Grayscale(3)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        l_size = max(sideX, sideY)\n",
    "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
    "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
    "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
    "        #MOD more useful cuts\n",
    "        if self.Overview > 0:\n",
    "            for i in range(self.Overview):\n",
    "                cutout = resize(pad_input, out_shape=output_shape)\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(gray(TF.hflip(cutout)))\n",
    "                    else:\n",
    "                        cutouts.append(gray(cutout))\n",
    "                else:\n",
    "                    if random.random() < 0.5:\n",
    "                        cutouts.append(TF.hflip(cutout))\n",
    "                    else:\n",
    "                        cutouts.append(cutout)\n",
    "\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
    "\n",
    "                              \n",
    "        if self.InnerCrop >0:\n",
    "            for i in range(self.InnerCrop):\n",
    "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
    "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "                offsety = torch.randint(0, sideY - size + 1, ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "                if random.random() < self.IC_Grey_P:\n",
    "                    cutout = gray(cutout)\n",
    "                cutout = resize(cutout, out_shape=output_shape)\n",
    "                cutouts.append(cutout)\n",
    "            if cutout_debug:\n",
    "                if is_colab:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"/content/cutout_InnerCrop.jpg\",quality=99)\n",
    "                else:\n",
    "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
    "        cutouts = torch.cat(cutouts)\n",
    "        if skip_augs is not True:\n",
    "            for i in range(cutouts.shape[0]):\n",
    "                cutouts[i]=self.augs(cutouts[i])\n",
    "        return cutouts\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def symmetry_transformation_fn(x):\n",
    "    if args.use_horizontal_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :, :w//2], torch.flip(x[:, :, :, :w//2], [-1])), -1)\n",
    "        print(\"horizontal symmetry applied\")\n",
    "    if args.use_vertical_symmetry:\n",
    "        [n, c, h, w] = x.size()\n",
    "        x = torch.concat((x[:, :, :h//2, :], torch.flip(x[:, :, :h//2, :], [-2])), -2)\n",
    "        print(\"vertical symmetry applied\")\n",
    "    return x\n",
    "\n",
    "def do_run():\n",
    "    seed = args.seed# + batch_num\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if args.init_image in ['','none', 'None', 'NONE']:\n",
    "        init_image = None\n",
    "    else:\n",
    "        init_image = args.init_image\n",
    "        init_scale = args.init_scale\n",
    "        skip_steps = args.skip_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    if args.prompts_series is not None:\n",
    "        prompts = args.prompts_series\n",
    "    else:\n",
    "        prompts = []\n",
    "\n",
    "    \n",
    "    if args.image_prompts_series is not None:\n",
    "        print(args.image_prompts_series)\n",
    "        image_prompts = args.image_prompts_series\n",
    "    else:\n",
    "        image_prompts = []\n",
    "\n",
    "    model_stats = []\n",
    "    for clip_model in clip_models:\n",
    "        cutn = 16\n",
    "        model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n",
    "        model_stat[\"clip_model\"] = clip_model\n",
    "\n",
    "        for prompt in prompts:\n",
    "            txt, weight = parse_prompt(prompt)\n",
    "            txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "\n",
    "            if args.fuzzy_prompt:\n",
    "                for i in range(25):\n",
    "                    model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n",
    "                    model_stat[\"weights\"].append(weight)\n",
    "            else:\n",
    "                model_stat[\"target_embeds\"].append(txt)\n",
    "                model_stat[\"weights\"].append(weight)\n",
    "\n",
    "        if image_prompts:\n",
    "            model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n",
    "            for prompt in image_prompt:\n",
    "                path, weight = parse_prompt(prompt)\n",
    "                img = Image.open(fetch(path)).convert('RGB')\n",
    "                img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
    "                batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "                embed = clip_model.encode_image(normalize(batch)).float()\n",
    "                if fuzzy_prompt:\n",
    "                    for i in range(25):\n",
    "                        model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n",
    "                        weights.extend([weight / cutn] * cutn)\n",
    "                else:\n",
    "                    model_stat[\"target_embeds\"].append(embed)\n",
    "                    model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
    "\n",
    "        model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
    "        model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
    "        if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
    "            raise RuntimeError('The weights must not sum to 0.')\n",
    "        model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
    "        model_stats.append(model_stat)\n",
    "\n",
    "        init = None\n",
    "        if init_image is not None:\n",
    "            init = Image.open(fetch(init_image)).convert('RGB')\n",
    "            init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
    "            init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    if args.perlin_init:\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        #init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, y=None):\n",
    "        with torch.enable_grad():\n",
    "            x_is_NaN = False\n",
    "            x = x.detach().requires_grad_()\n",
    "            n = x.shape[0]\n",
    "            if use_secondary_model is True:\n",
    "                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "                cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
    "                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "            else:\n",
    "                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
    "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "                x_in_grad = torch.zeros_like(x_in)\n",
    "                \n",
    "            #MOD this whole chunk and spliting off cuts outer/inner:\n",
    "            normalizeGuideToRes = width_height[0] * width_height[1] / 425984.0 #Normalize to 832,512 res\n",
    "                \n",
    "            for model_stat in model_stats:\n",
    "                t_int = int(t.item())+1 #errors on last step without +1, need to find source\n",
    "                #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
    "                try:\n",
    "                    input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n",
    "                except:\n",
    "                    input_resolution=224\n",
    "                for i in range(args.cutn_batches[1000-t_int]):\n",
    "                    cuts = MakeCutoutsDango(input_resolution,\n",
    "                            Overview= args.cut_overview[1000-t_int], \n",
    "                            InnerCrop =args.cut_innercut[1000-t_int],\n",
    "                            IC_Size_Pow=args.cut_ic_pow[1000-t_int],\n",
    "                            IC_Grey_P = args.cut_icgray_p[1000-t_int]\n",
    "                            )\n",
    "                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
    "                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
    "                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
    "                    dists = dists.view([args.cut_overview[1000-t_int] + args.cut_innercut[1000-t_int], n, -1])\n",
    "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
    "                    #loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n",
    "                    x_in_grad += torch.autograd.grad(losses.sum() * args.clip_guidance_scale[1000-t_int], x_in)[0] / args.cutn_batches[1000-t_int]\n",
    "            #tv_losses = tv_loss(x_in)\n",
    "            #if use_secondary_model is True:\n",
    "                #range_losses = range_loss(out)\n",
    "            #else:\n",
    "                #range_losses = range_loss(out['pred_xstart'])\n",
    "            #sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
    "            #loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "            #if init is not None and init_scale:\n",
    "                #init_losses = lpips_model(x_in, init)\n",
    "                #loss = loss + init_losses.sum() * init_scale\n",
    "            #x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "            if torch.isnan(x_in_grad).any()==False:\n",
    "                grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "            else:\n",
    "                # print(\"NaN'd\")\n",
    "                x_is_NaN = True\n",
    "                grad = torch.zeros_like(x)\n",
    "        if args.clamp_grad and x_is_NaN == False:\n",
    "            magnitude = grad.square().mean().sqrt()\n",
    "            grad = grad * magnitude.clamp(max=args.clamp_max[1000 - t_int]) / magnitude  #min=-0.02, min=-clamp_max, \n",
    "        #MOD:    \n",
    "        #Static thresholding: We refer to elementwise clipping the x-prediction to [1, 1] as static thresholding. This method was in fact used but not emphasized in previous work [28 ], and to our knowledge its importance has not been investigated in the context of guided sampling. We discover that static thresholding is essential to sampling with large guidance weights and prevents generation of blank images. Nonetheless, static thresholding still results in over-saturated and less detailed images as the guidance weight further increases.\n",
    "        #made dynamic clamp instead of regular clamp\n",
    "        return grad.div(max(grad.max(), 0.0 - grad.min(), 1.0))\n",
    "\n",
    "    if args.diffusion_sampling_mode == 'ddim':\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.plms_sample_loop_progressive\n",
    "\n",
    "\n",
    "    image_display = Output()\n",
    "    for i in range(args.n_batches):\n",
    "        display.clear_output(wait=True)\n",
    "        batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n",
    "        batchBar.n = i\n",
    "        batchBar.refresh()\n",
    "        print('')\n",
    "        display.display(image_display)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        cur_t = diffusion.num_timesteps - skip_steps - 1\n",
    "        total_steps = cur_t\n",
    "\n",
    "        if perlin_init:\n",
    "            init = regen_perlin()\n",
    "\n",
    "        if args.diffusion_sampling_mode == 'ddim':\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                eta=args.eta,\n",
    "                transformation_fn=symmetry_transformation_fn,\n",
    "                transformation_percent=args.transformation_percent\n",
    "            )\n",
    "        else:\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, args.side_y, args.side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_steps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                order=2,\n",
    "        )\n",
    "\n",
    "        # with run_display:\n",
    "        # display.clear_output(wait=True)\n",
    "        for j, sample in enumerate(samples):    \n",
    "            cur_t -= 1\n",
    "            intermediateStep = False\n",
    "            if args.steps_per_checkpoint is not None:\n",
    "                if j % steps_per_checkpoint == 0 and j > 0:\n",
    "                    intermediateStep = True\n",
    "            elif j in args.intermediate_saves:\n",
    "                intermediateStep = True\n",
    "            with image_display:\n",
    "                if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
    "                    for k, image in enumerate(sample['pred_xstart']):\n",
    "                        # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                        current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
    "                        percent = math.ceil(j/total_steps*100)\n",
    "                        if args.n_batches > 0:\n",
    "                            #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n",
    "                            if cur_t == -1 and args.intermediates_in_subfolder is True:\n",
    "                                save_num = f'{frame_num:04}' if args.animation_mode != \"None\" else i\n",
    "                                filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n",
    "                            else:\n",
    "                                #If we're working with percentages, append it\n",
    "                                if args.steps_per_checkpoint is not None:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n",
    "                                # Or else, iIf we're working with specific steps, append those\n",
    "                                else:\n",
    "                                    filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n",
    "                        image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
    "                        if j % args.display_rate == 0 or cur_t == -1:\n",
    "                            image.save('progress.png')\n",
    "                            display.clear_output(wait=True)\n",
    "                            display.display(display.Image('progress.png'))\n",
    "                        if args.steps_per_checkpoint is not None:\n",
    "                            if j % args.steps_per_checkpoint == 0 and j > 0:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        else:\n",
    "                            if j in args.intermediate_saves:\n",
    "                                if args.intermediates_in_subfolder is True:\n",
    "                                    image.save(f'{partialFolder}/{filename}')\n",
    "                                else:\n",
    "                                    image.save(f'{batchFolder}/{filename}')\n",
    "                        if cur_t == -1:\n",
    "                            save_settings()\n",
    "                            image.save(f'{batchFolder}/{filename}')\n",
    "                    \n",
    "      #plt.plot(np.array(loss_values), 'r')\n",
    "\n",
    "def save_settings():\n",
    "    setting_list = {\n",
    "        'text_prompts': text_prompts,\n",
    "        'image_prompts': image_prompts,\n",
    "        'clip_guidance_scale': clip_guidance_scale,\n",
    "        'tv_scale': tv_scale,\n",
    "        'range_scale': range_scale,\n",
    "        'sat_scale': sat_scale,\n",
    "        'cutn_batches': cutn_batches,\n",
    "        'init_image': init_image,\n",
    "        'init_scale': init_scale,\n",
    "        'skip_steps': skip_steps,\n",
    "        'perlin_init': perlin_init,\n",
    "        'perlin_mode': perlin_mode,\n",
    "        'skip_augs': skip_augs,\n",
    "        'randomize_class': randomize_class,\n",
    "        'clip_denoised': clip_denoised,\n",
    "        'clamp_grad': clamp_grad,\n",
    "        'clamp_max': clamp_max,\n",
    "        'seed': seed,\n",
    "        'fuzzy_prompt': fuzzy_prompt,\n",
    "        'rand_mag': rand_mag,\n",
    "        'eta': eta,\n",
    "        'width': width_height[0],\n",
    "        'height': width_height[1],\n",
    "        'diffusion_model': diffusion_model,\n",
    "        'use_secondary_model': use_secondary_model,\n",
    "        'steps': steps,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "        'ViTB32': ViTB32,\n",
    "        'ViTB16': ViTB16,\n",
    "        'ViTL14': ViTL14,\n",
    "        'ViTL14_336px': ViTL14_336px,\n",
    "        'RN101': RN101,\n",
    "        'RN50': RN50,\n",
    "        'RN50x4': RN50x4,\n",
    "        'RN50x16': RN50x16,\n",
    "        'RN50x64': RN50x64,\n",
    "        'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n",
    "        'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n",
    "        'ViTB32_laion400m_32': ViTB32_laion400m_32,\n",
    "        'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n",
    "        'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n",
    "        'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n",
    "        'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n",
    "        'RN50_yffcc15m': RN50_yffcc15m,\n",
    "        'RN50_cc12m': RN50_cc12m,\n",
    "        'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n",
    "        'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n",
    "        'RN101_yfcc15m': RN101_yfcc15m,\n",
    "        'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n",
    "        'cut_overview': str(cut_overview),\n",
    "        'cut_innercut': str(cut_innercut),\n",
    "        'cut_ic_pow': str(cut_ic_pow),\n",
    "        'cut_icgray_p': str(cut_icgray_p),\n",
    "        'sampling_mode': sampling_mode,\n",
    "        'use_horizontal_symmetry':use_horizontal_symmetry,\n",
    "        'use_vertical_symmetry':use_vertical_symmetry,\n",
    "        'transformation_percent':transformation_percent,\n",
    "    }\n",
    "    # print('Settings:', setting_list)\n",
    "    with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
    "        json.dump(setting_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "DefSecModel"
   },
   "outputs": [],
   "source": [
    "#@title 1.6 Define the secondary diffusion model\n",
    "\n",
    "def append_dims(x, n):\n",
    "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
    "\n",
    "\n",
    "def expand_to_planes(x, shape):\n",
    "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
    "\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
    "\n",
    "\n",
    "def t_to_alpha_sigma(t):\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionOutput:\n",
    "    v: torch.Tensor\n",
    "    pred: torch.Tensor\n",
    "    eps: torch.Tensor\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, c),\n",
    "            ConvBlock(c, c),\n",
    "            SkipBlock([\n",
    "                nn.AvgPool2d(2),\n",
    "                ConvBlock(c, c * 2),\n",
    "                ConvBlock(c * 2, c * 2),\n",
    "                SkipBlock([\n",
    "                    nn.AvgPool2d(2),\n",
    "                    ConvBlock(c * 2, c * 4),\n",
    "                    ConvBlock(c * 4, c * 4),\n",
    "                    SkipBlock([\n",
    "                        nn.AvgPool2d(2),\n",
    "                        ConvBlock(c * 4, c * 8),\n",
    "                        ConvBlock(c * 8, c * 4),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                    ]),\n",
    "                    ConvBlock(c * 8, c * 4),\n",
    "                    ConvBlock(c * 4, c * 2),\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                ]),\n",
    "                ConvBlock(c * 4, c * 2),\n",
    "                ConvBlock(c * 2, c),\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            ]),\n",
    "            ConvBlock(c * 2, c),\n",
    "            nn.Conv2d(c, 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)\n",
    "\n",
    "\n",
    "class SecondaryDiffusionImageNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.down = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, cs[0]),\n",
    "            ConvBlock(cs[0], cs[0]),\n",
    "            SkipBlock([\n",
    "                self.down,\n",
    "                ConvBlock(cs[0], cs[1]),\n",
    "                ConvBlock(cs[1], cs[1]),\n",
    "                SkipBlock([\n",
    "                    self.down,\n",
    "                    ConvBlock(cs[1], cs[2]),\n",
    "                    ConvBlock(cs[2], cs[2]),\n",
    "                    SkipBlock([\n",
    "                        self.down,\n",
    "                        ConvBlock(cs[2], cs[3]),\n",
    "                        ConvBlock(cs[3], cs[3]),\n",
    "                        SkipBlock([\n",
    "                            self.down,\n",
    "                            ConvBlock(cs[3], cs[4]),\n",
    "                            ConvBlock(cs[4], cs[4]),\n",
    "                            SkipBlock([\n",
    "                                self.down,\n",
    "                                ConvBlock(cs[4], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[4]),\n",
    "                                self.up,\n",
    "                            ]),\n",
    "                            ConvBlock(cs[4] * 2, cs[4]),\n",
    "                            ConvBlock(cs[4], cs[3]),\n",
    "                            self.up,\n",
    "                        ]),\n",
    "                        ConvBlock(cs[3] * 2, cs[3]),\n",
    "                        ConvBlock(cs[3], cs[2]),\n",
    "                        self.up,\n",
    "                    ]),\n",
    "                    ConvBlock(cs[2] * 2, cs[2]),\n",
    "                    ConvBlock(cs[2], cs[1]),\n",
    "                    self.up,\n",
    "                ]),\n",
    "                ConvBlock(cs[1] * 2, cs[1]),\n",
    "                ConvBlock(cs[1], cs[0]),\n",
    "                self.up,\n",
    "            ]),\n",
    "            ConvBlock(cs[0] * 2, cs[0]),\n",
    "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffClipSetTop"
   },
   "source": [
    "# 2. Diffusion and CLIP model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ModelSettings"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512x512_diffusion_uncond_finetune_008100 already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "secondary already downloaded. If the file is corrupt, enable check_model_SHA.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\Zhenya\\.conda\\envs\\DiscoDiffusionTanuki\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#@markdown ####**Models Settings (note: For pixel art, the best is pixelartdiffusion_expanded):**\n",
    "diffusion_model = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\", \"portrait_generator_v001\", \"pixelartdiffusion_expanded\", \"pixel_art_diffusion_hard_256\", \"pixel_art_diffusion_soft_256\", \"pixelartdiffusion4k\", \"watercolordiffusion_2\", \"watercolordiffusion\", \"PulpSciFiDiffusion\", \"custom\"]\n",
    "\n",
    "use_secondary_model = True #@param {type: 'boolean'}\n",
    "diffusion_sampling_mode = 'ddim' #@param ['plms','ddim']\n",
    "#@markdown #####**Custom model:**\n",
    "custom_path = '/content/drive/MyDrive/deep_learning/ddpm/ema_0.9999_058000.pt'#@param {type: 'string'}\n",
    "\n",
    "#@markdown #####**CLIP settings:**\n",
    "use_checkpoint = True #@param {type: 'boolean'}\n",
    "ViTB32 = True #@param{type:\"boolean\"}\n",
    "ViTB16 = False #@param{type:\"boolean\"}\n",
    "ViTL14 = True #@param{type:\"boolean\"}\n",
    "ViTL14_336px = False #@param{type:\"boolean\"}\n",
    "RN101 = True #@param{type:\"boolean\"}\n",
    "RN50 = False #@param{type:\"boolean\"}\n",
    "RN50x4 = False #@param{type:\"boolean\"}\n",
    "RN50x16 = True #@param{type:\"boolean\"}\n",
    "RN50x64 = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown #####**OpenCLIP settings:**\n",
    "ViTB32_laion2b_e16 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32_laion400m_32 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB32quickgelu_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e31 = False #@param{type:\"boolean\"}\n",
    "ViTB16_laion400m_e32 = False #@param{type:\"boolean\"}\n",
    "RN50_yffcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_cc12m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN50_quickgelu_cc12m = False #@param{type:\"boolean\"}\n",
    "RN101_yfcc15m = False #@param{type:\"boolean\"}\n",
    "RN101_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown If you're having issues with model downloads, check this to compare SHA's:\n",
    "check_model_SHA = False #@param{type:\"boolean\"}\n",
    "\n",
    "diff_model_map = {\n",
    "    '256x256_diffusion_uncond': { 'downloaded': False, 'sha': 'a37c32fffd316cd494cf3f35b339936debdc1576dad13fe57c42399a5dbc78b1', 'uri_list': ['https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt', 'https://www.dropbox.com/s/9tqnqo930mpnpcn/256x256_diffusion_uncond.pt'] },\n",
    "    '512x512_diffusion_uncond_finetune_008100': { 'downloaded': False, 'sha': '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648', 'uri_list': ['https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt', 'https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt'] },\n",
    "    'portrait_generator_v001': { 'downloaded': False, 'sha': 'b7e8c747af880d4480b6707006f1ace000b058dd0eac5bb13558ba3752d9b5b9', 'uri_list': ['https://huggingface.co/felipe3dartist/portrait_generator_v001/resolve/main/portrait_generator_v001_ema_0.9999_1MM.pt'] },\n",
    "    'pixelartdiffusion_expanded': { 'downloaded': False, 'sha': 'a73b40556634034bf43b5a716b531b46fb1ab890634d854f5bcbbef56838739a', 'uri_list': ['https://huggingface.co/KaliYuga/PADexpanded/resolve/main/PADexpanded.pt'] },\n",
    "    'pixel_art_diffusion_hard_256': { 'downloaded': False, 'sha': 'be4a9de943ec06eef32c65a1008c60ad017723a4d35dc13169c66bb322234161', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_hard_256/resolve/main/pixel_art_diffusion_hard_256.pt'] },\n",
    "    'pixel_art_diffusion_soft_256': { 'downloaded': False, 'sha': 'd321590e46b679bf6def1f1914b47c89e762c76f19ab3e3392c8ca07c791039c', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_soft_256/resolve/main/pixel_art_diffusion_soft_256.pt'] },\n",
    "    'pixelartdiffusion4k': { 'downloaded': False, 'sha': 'a1ba4f13f6dabb72b1064f15d8ae504d98d6192ad343572cc416deda7cccac30', 'uri_list': ['https://huggingface.co/KaliYuga/pixelartdiffusion4k/resolve/main/pixelartdiffusion4k.pt'] },\n",
    "    'watercolordiffusion_2': { 'downloaded': False, 'sha': '49c281b6092c61c49b0f1f8da93af9b94be7e0c20c71e662e2aa26fee0e4b1a9', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion_2/resolve/main/watercolordiffusion_2.pt'] },\n",
    "    'watercolordiffusion': { 'downloaded': False, 'sha': 'a3e6522f0c8f278f90788298d66383b11ac763dd5e0d62f8252c962c23950bd6', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion/resolve/main/watercolordiffusion.pt'] },\n",
    "    'PulpSciFiDiffusion': { 'downloaded': False, 'sha': 'b79e62613b9f50b8a3173e5f61f0320c7dbb16efad42a92ec94d014f6e17337f', 'uri_list': ['https://huggingface.co/KaliYuga/PulpSciFiDiffusion/resolve/main/PulpSciFiDiffusion.pt'] },\n",
    "    'secondary': { 'downloaded': False, 'sha': '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a', 'uri_list': ['https://the-eye.eu/public/AI/models/v-diffusion/secondary_model_imagenet_2.pth', 'https://ipfs.pollinations.ai/ipfs/bafybeibaawhhk7fhyhvmm7x24zwwkeuocuizbqbcg5nqx64jq42j75rdiy/secondary_model_imagenet_2.pth'] },\n",
    "}\n",
    "\n",
    "kaliyuga_pixel_art_model_names = ['pixelartdiffusion_expanded', 'pixel_art_diffusion_hard_256', 'pixel_art_diffusion_soft_256', 'pixelartdiffusion4k', 'PulpSciFiDiffusion']\n",
    "kaliyuga_watercolor_model_names = ['watercolordiffusion', 'watercolordiffusion_2']\n",
    "kaliyuga_pulpscifi_model_names = ['PulpSciFiDiffusion']\n",
    "diffusion_models_256x256_list = ['256x256_diffusion_uncond'] + kaliyuga_pixel_art_model_names + kaliyuga_watercolor_model_names + kaliyuga_pulpscifi_model_names\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_model_filename(diffusion_model_name):\n",
    "    model_uri = diff_model_map[diffusion_model_name]['uri_list'][0]\n",
    "    model_filename = os.path.basename(urlparse(model_uri).path)\n",
    "    return model_filename\n",
    "\n",
    "\n",
    "def download_model(diffusion_model_name, uri_index=0):\n",
    "    if diffusion_model_name != 'custom':\n",
    "        model_filename = get_model_filename(diffusion_model_name)\n",
    "        model_local_path = os.path.join(model_path, model_filename)\n",
    "        if os.path.exists(model_local_path) and check_model_SHA:\n",
    "            print(f'Checking {diffusion_model_name} File')\n",
    "            with open(model_local_path, \"rb\") as f:\n",
    "                bytes = f.read() \n",
    "                hash = hashlib.sha256(bytes).hexdigest()\n",
    "            if hash == diff_model_map[diffusion_model_name]['sha']:\n",
    "                print(f'{diffusion_model_name} SHA matches')\n",
    "                diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "            else:\n",
    "                print(f\"{diffusion_model_name} SHA doesn't match. Will redownload it.\")\n",
    "        elif os.path.exists(model_local_path) and not check_model_SHA or diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            print(f'{diffusion_model_name} already downloaded. If the file is corrupt, enable check_model_SHA.')\n",
    "            diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "\n",
    "        if not diff_model_map[diffusion_model_name]['downloaded']:\n",
    "            for model_uri in diff_model_map[diffusion_model_name]['uri_list']:\n",
    "                fetchsave(model_uri, model_local_path)\n",
    "                if os.path.exists(model_local_path):\n",
    "                    diff_model_map[diffusion_model_name]['downloaded'] = True\n",
    "                    return\n",
    "                else:\n",
    "                    print(f'{diffusion_model_name} model download from {model_uri} failed. Will try any fallback uri.')\n",
    "            print(f'{diffusion_model_name} download failed.')\n",
    "\n",
    "\n",
    "# Download the diffusion model(s)\n",
    "download_model(diffusion_model)\n",
    "if use_secondary_model:\n",
    "    download_model('secondary')\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == '256x256_diffusion_uncond':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
    "        'image_size': 256,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': not useCPU,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "elif diffusion_model == 'portrait_generator_v001':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': 1000,\n",
    "        'rescale_timesteps': True,\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 128,\n",
    "        'num_heads': 4,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_checkpoint': use_checkpoint,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "    })\n",
    "else:  # E.g. A model finetuned by KaliYuga\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "    \n",
    "if diffusion_model == 'custom':\n",
    "    model_config.update({\n",
    "          'attention_resolutions': '16',\n",
    "          'class_cond': False,\n",
    "          'diffusion_steps': 1000,\n",
    "          'rescale_timesteps': True,\n",
    "          'timestep_respacing': 'ddim100',\n",
    "          'image_size': 256,\n",
    "          'learn_sigma': True,\n",
    "          'noise_schedule': 'linear',\n",
    "          'num_channels': 128,\n",
    "          'num_heads': 1,\n",
    "          'num_res_blocks': 2,\n",
    "          'use_checkpoint': use_checkpoint,\n",
    "          'use_fp16': True,\n",
    "          'use_scale_shift_norm': False,\n",
    "      })\n",
    "\n",
    "model_default = model_config['image_size']\n",
    "\n",
    "if use_secondary_model:\n",
    "    secondary_model = SecondaryDiffusionImageNet2()\n",
    "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "    secondary_model.eval().requires_grad_(False).to(device)\n",
    "\n",
    "clip_models = []\n",
    "if ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion2b_e16: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion2b_e16').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32_laion400m_32: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB32quickgelu_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\n",
    "if ViTB16_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\n",
    "if RN50_yffcc15m: clip_models.append(open_clip.create_model('RN50', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_cc12m: clip_models.append(open_clip.create_model('RN50', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN50_quickgelu_cc12m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='cc12m').eval().requires_grad_(False).to(device))\n",
    "if RN101_yfcc15m: clip_models.append(open_clip.create_model('RN101', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "if RN101_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN101-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n",
    "\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SettingsTop"
   },
   "source": [
    "# 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BasicSettings"
   },
   "outputs": [],
   "source": [
    "#@markdown ####**Basic Settings:**\n",
    "batch_name = 'DDT' #@param{type: 'string'}\n",
    "display_rate = 20 #@param{type: 'number'}\n",
    "n_batches = 5 #@param{type: 'number'}\n",
    "steps = 250 #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\n",
    "width_height_for_512x512_models = [960, 576] #@param{type: 'raw'}\n",
    "clip_guidance_scale = \"[15000]*100+[16000]*400+[12000]*499+[10000]*1\" #@param{type: 'string'}\n",
    "tv_scale = 0#@param{type: 'number'}\n",
    "range_scale = 150#@param{type: 'number'}\n",
    "sat_scale = 0#@param{type: 'number'}\n",
    "cutn_batches = \"[2]*200+[2]*200+[4]*200+[4]*200+[4]*200\"#@param{type: 'string'}\n",
    "skip_augs = False#@param{type: 'boolean'}\n",
    "\n",
    "#@markdown ####**Image dimensions to be used for 256x256 models (e.g. pixelart models):**\n",
    "width_height_for_256x256_models = [512, 448] #@param{type: 'raw'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Init Image Settings:**\n",
    "init_image = None #@param{type: 'string'}\n",
    "init_scale = 1000 #@param{type: 'integer'}\n",
    "skip_steps = 0 #@param{type: 'integer'}\n",
    "#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n",
    "\n",
    "width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "\n",
    "#Get corrected sizes\n",
    "side_x = (width_height[0]//64)*64;\n",
    "side_y = (width_height[1]//64)*64;\n",
    "if side_x != width_height[0] or side_y != width_height[1]:\n",
    "    print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "\n",
    "#Make folder for batch\n",
    "batchFolder = f'{outDirPath}/{batch_name}'\n",
    "createPath(batchFolder)\n",
    "#@markdown ####**Saving:**\n",
    "\n",
    "intermediate_saves = [50, 100, 150, 200]#@param{type: 'raw'}\n",
    "intermediates_in_subfolder = True #@param{type: 'boolean'}\n",
    "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
    "\n",
    "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
    "\n",
    "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
    "\n",
    "\n",
    "if type(intermediate_saves) is not list:\n",
    "    if intermediate_saves:\n",
    "        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "        print(f'Will save every {steps_per_checkpoint} steps')\n",
    "    else:\n",
    "        steps_per_checkpoint = steps+10\n",
    "else:\n",
    "    steps_per_checkpoint = None\n",
    "\n",
    "if intermediate_saves and intermediates_in_subfolder is True:\n",
    "    partialFolder = f'{batchFolder}/partials'\n",
    "    createPath(partialFolder)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Advanced Settings:**\n",
    "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
    "\n",
    "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
    "\n",
    "perlin_init = True  #@param{type: 'boolean'}\n",
    "perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\n",
    "set_seed = 'random_seed' #@param{type: 'string'}\n",
    "eta = \"[0.0]*200+[0.001]*799+[0.36]*1\"#@param{type: 'string'}\n",
    "clamp_grad = True #@param{type: 'boolean'}\n",
    "clamp_max = \"[0.064]*999+[0.088]*1\" #@param{type: 'string'}\n",
    "frames_skip_steps = \"0%\"#@param{type: 'string'}\n",
    "\n",
    "### EXTRA ADVANCED SETTINGS:\n",
    "randomize_class = True\n",
    "clip_denoised = False\n",
    "fuzzy_prompt = False\n",
    "rand_mag = 0.05\n",
    "sampling_mode = \"ddim\"#@param{type: 'string'}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Cutn Scheduling:**\n",
    "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
    "\n",
    "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
    "\n",
    "cut_overview = \"[6]*200+[5]*200+[2]*200+[0]*200+[0]*200\" #@param {type: 'string'}       \n",
    "cut_innercut = \"[0]*200+[1]*200+[4]*200+[6]*200+[6]*200\" #@param {type: 'string'}  \n",
    "cut_ic_pow = \"[1]*200+[4]*200+[8]*200+[16]*200+[32]*100+[88]*100\" #@param {type: 'string'}  \n",
    "cut_icgray_p = \"[0.21]*150+[0.2]*350+[0]*100+[0]*100+[0]*300\" #@param {type: 'string'}\n",
    "\n",
    "#@markdown KaliYuga model settings. Refer to [cut_ic_pow](https://ezcharts.miraheze.org/wiki/Category:Cut_ic_pow) as a guide. Values between 1 and 100 all work.\n",
    "pad_or_pulp_cut_overview = \"[15]*100+[15]*100+[12]*100+[12]*100+[6]*100+[4]*100+[2]*200+[0]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_innercut = \"[1]*100+[1]*100+[4]*100+[4]*100+[8]*100+[8]*100+[10]*200+[10]*200\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "pad_or_pulp_cut_icgray_p = \"[0.87]*100+[0.78]*50+[0.73]*50+[0.64]*60+[0.56]*40+[0.50]*50+[0.33]*100+[0.19]*150+[0]*400\" #@param {type: 'string'}\n",
    "\n",
    "watercolor_cut_overview = \"[14]*200+[12]*200+[4]*400+[0]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_innercut = \"[2]*200+[4]*200+[12]*400+[12]*200\" #@param {type: 'string'}\n",
    "watercolor_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\n",
    "watercolor_cut_icgray_p = \"[0.7]*100+[0.6]*100+[0.45]*100+[0.3]*100+[0]*600\" #@param {type: 'string'}\n",
    "\n",
    "if (diffusion_model in kaliyuga_pixel_art_model_names) or (diffusion_model in kaliyuga_pulpscifi_model_names):\n",
    "    cut_overview = pad_or_pulp_cut_overview\n",
    "    cut_innercut = pad_or_pulp_cut_innercut\n",
    "    cut_ic_pow = pad_or_pulp_cut_ic_pow\n",
    "    cut_icgray_p = pad_or_pulp_cut_icgray_p\n",
    "elif diffusion_model in kaliyuga_watercolor_model_names:\n",
    "    cut_overview = watercolor_cut_overview\n",
    "    cut_innercut = watercolor_cut_innercut\n",
    "    cut_ic_pow = watercolor_cut_ic_pow\n",
    "    cut_icgray_p = watercolor_cut_icgray_p\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Transformation Settings:**\n",
    "use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
    "use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
    "transformation_percent = [0.09] #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    #Modifying settings for local notebook easier here\n",
    "    batch_name = 'DDT'\n",
    "    n_batches = 5\n",
    "    width_height_for_512x512_models = [960,576]\n",
    "    steps = 250\n",
    "    set_seed = 'random_seed'\n",
    "    display_rate = 20\n",
    "    intermediate_saves = [50, 100, 150, 200]\n",
    "    intermediates_in_subfolder = True\n",
    "    clip_guidance_scale = \"[15000]*100+[16000]*400+[12000]*499+[10000]*1\"\n",
    "    frames_skip_steps = \"0%\"\n",
    "    tv_scale = 0\n",
    "    range_scale = 150\n",
    "    sat_scale = 0\n",
    "    cutn_batches = \"[2]*200+[2]*200+[4]*200+[4]*200+[4]*200\"\n",
    "    cut_overview = \"[6]*200+[5]*200+[2]*200+[0]*200+[0]*200\"      \n",
    "    cut_innercut = \"[0]*200+[1]*200+[4]*200+[6]*200+[6]*200\"\n",
    "    cut_ic_pow = \"[1]*200+[4]*200+[8]*200+[16]*200+[32]*100+[88]*100\"\n",
    "    cut_icgray_p = \"[0.21]*150+[0.2]*350+[0]*100+[0]*100+[0]*300\"\n",
    "    clamp_grad = True\n",
    "    clamp_max = \"[0.064]*999+[0.088]*1\"\n",
    "    eta = \"[0.0]*200+[0.001]*799+[0.36]*1\"\n",
    "    perlin_init = True\n",
    "    perlin_mode = 'mixed'\n",
    "    \n",
    "    skip_augs = False\n",
    "    use_vertical_symmetry = False\n",
    "    use_horizontal_symmetry = False\n",
    "    transformation_percent = [0.09]\n",
    "\n",
    "    #Settings cleanup\n",
    "    width_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n",
    "    side_x = (width_height[0]//64)*64;\n",
    "    side_y = (width_height[1]//64)*64;\n",
    "    if side_x != width_height[0] or side_y != width_height[1]:\n",
    "        print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "    batchFolder = f'{outDirPath}/{batch_name}'\n",
    "    createPath(batchFolder)\n",
    "    if type(intermediate_saves) is not list:\n",
    "        if intermediate_saves:\n",
    "            steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "            steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "            print(f'Will save every {steps_per_checkpoint} steps')\n",
    "        else:\n",
    "            steps_per_checkpoint = steps+10\n",
    "    else:\n",
    "        steps_per_checkpoint = None\n",
    "    if intermediate_saves and intermediates_in_subfolder is True:\n",
    "        partialFolder = f'{batchFolder}/partials'\n",
    "        createPath(partialFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PromptsTop"
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Prompts"
   },
   "outputs": [],
   "source": [
    "# Note: If using a pixelart diffusion model, try adding \"#pixelart\" to the end of the prompt for a stronger effect. It'll tend to work a lot better!\n",
    "text_prompts = {\n",
    "    0: [\"Beautiful ultradetailed matte painting of a single ancient Egyptian pyramid that glows with a gentle noble aura surrounded by statues and obelisks and monuments to dragon gods, by federico pelat and john howe and sparth, trending on artstation, ultrawide lens:2\",\n",
    "            \"people, dof, blur:-1\",\n",
    "       ],\n",
    "}\n",
    "\n",
    "image_prompts = {\n",
    "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffuseTop"
   },
   "source": [
    "# 4. Diffuse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoTheRun"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cd4b5c942846ae8cedf345dc3effeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e800b290b3fb438787aa9bb960a5d59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57db98f0ea414fa8944f7c1c98263823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Do the Run!\n",
    "#@markdown `n_batches` ignored with animation modes.\n",
    "\n",
    "#Update Model Settings\n",
    "timestep_respacing = f'ddim{steps}'\n",
    "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
    "model_config.update({\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "})\n",
    "\n",
    "batch_size = 1 \n",
    "\n",
    "def move_files(start_num, end_num, old_folder, new_folder):\n",
    "    for i in range(start_num, end_num):\n",
    "        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "resume_run = False #@param{type: 'boolean'}\n",
    "run_to_resume = 'latest' #@param{type: 'string'}\n",
    "resume_from_frame = 'latest' #@param{type: 'string'}\n",
    "retain_overwritten_frames = False #@param{type: 'boolean'}\n",
    "if retain_overwritten_frames:\n",
    "    retainFolder = f'{batchFolder}/retained'\n",
    "    createPath(retainFolder)\n",
    "\n",
    "\n",
    "skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
    "calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
    "\n",
    "if steps <= calc_frames_skip_steps:\n",
    "    sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
    "\n",
    "start_frame = 0\n",
    "batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
    "while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\"):\n",
    "    batchNum += 1\n",
    "\n",
    "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
    "\n",
    "if set_seed == 'random_seed':\n",
    "    random.seed()\n",
    "    seed = random.randint(0, 2**32)\n",
    "    # print(f'Using seed: {seed}')\n",
    "else:\n",
    "    seed = int(set_seed)\n",
    "\n",
    "    \n",
    "args = {\n",
    "    'batchNum': batchNum,\n",
    "    'prompts_series': text_prompts[0] if text_prompts else None,\n",
    "    'image_prompts_series':image_prompts[0] if image_prompts else None,\n",
    "    'seed': seed,\n",
    "    'display_rate':display_rate,\n",
    "    'n_batches':n_batches,\n",
    "    'batch_size':batch_size,\n",
    "    'batch_name': batch_name,\n",
    "    'steps': steps,\n",
    "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "    'width_height': width_height,\n",
    "    'clip_guidance_scale': smooth_jazz(eval(clip_guidance_scale)),\n",
    "    'tv_scale': tv_scale,\n",
    "    'range_scale': range_scale,\n",
    "    'sat_scale': sat_scale,\n",
    "    'cutn_batches': smooth_jazz(eval(cutn_batches)),\n",
    "    'init_image': init_image,\n",
    "    'init_scale': init_scale,\n",
    "    'skip_steps': skip_steps,\n",
    "    'side_x': side_x,\n",
    "    'side_y': side_y,\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'skip_step_ratio': skip_step_ratio,\n",
    "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
    "    'text_prompts': text_prompts,\n",
    "    'image_prompts': image_prompts,\n",
    "    'cut_overview': smooth_jazz(eval(cut_overview)),\n",
    "    'cut_innercut': smooth_jazz(eval(cut_innercut)),\n",
    "    'cut_ic_pow': smooth_jazz(eval(cut_ic_pow)),\n",
    "    'cut_icgray_p': smooth_jazz(eval(cut_icgray_p)),\n",
    "    'intermediate_saves': intermediate_saves,\n",
    "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
    "    'steps_per_checkpoint': steps_per_checkpoint,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'set_seed': set_seed,\n",
    "    'eta': smooth_jazz(eval(eta)),\n",
    "    'clamp_grad': clamp_grad,\n",
    "    'clamp_max': smooth_jazz(eval(clamp_max)),\n",
    "    'skip_augs': skip_augs,\n",
    "    'randomize_class': randomize_class,\n",
    "    'clip_denoised': clip_denoised,\n",
    "    'fuzzy_prompt': fuzzy_prompt,\n",
    "    'rand_mag': rand_mag,\n",
    "    'use_vertical_symmetry': use_vertical_symmetry,\n",
    "    'use_horizontal_symmetry': use_horizontal_symmetry,\n",
    "    'transformation_percent': transformation_percent,\n",
    "    'animation_mode': 'None',\n",
    "}\n",
    "\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "print('Prepping model...')\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "if diffusion_model == 'custom':\n",
    "    model.load_state_dict(torch.load(custom_path, map_location='cpu'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    do_run()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print('Seed used:', seed)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "CreditsChTop",
    "TutorialTop",
    "CheckGPU",
    "InstallDeps",
    "DefMidasFns",
    "DefFns",
    "DefSecModel",
    "DefSuperRes",
    "AnimSetTop",
    "ExtraSetTop",
    "InstallRAFT",
    "CustModel",
    "FlowFns1",
    "FlowFns2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Disco Diffusion v5.6 [Now with portrait_generator_v001]",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
